<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>操作语义</title>
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/main.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
</head>
<body>
<!-- Header start -->
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="#">TensorFlow</a>
    <button class="navbar-toggler" type="button" aria-expanded="false" aria-label="Menu"
            onclick="$('.collapse').toggle()">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse">
        <ul class="navbar-nav mr-auto">
        </ul>
        <!-- TODO: Search function-->
        <!--<form class="form-inline my-2 my-lg-0">-->
            <!--<input class="form-control mr-sm-2" type="search" placeholder="Search" aria-label="Search">-->
            <!--<button class="btn btn-outline-success my-2 my-sm-0" type="submit">Search</button>-->
        <!--</form>-->
    </div>
</nav>
<script>
    var head = [{'link': '//xitu.github.io/tensorflow-docs-web/extend/index.html', 'name': '扩展', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/install/index.html', 'name': '安装 TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/deploy/index.html', 'name': '部署', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/about/index.html', 'name': 'About TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/index.html', 'name': '开始', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/index.html', 'name': '概述', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/tutorials/index.html', 'name': '教程', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/javascript/index.html', 'name': 'JavaScript', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/index.html', 'name': '性能', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/community/index.html', 'name': 'Community', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/programmers_guide/index.html', 'name': '开发者指南', 'selected': 0}]
</script>
<!-- Header end -->

<!-- Content start-->
<div class="container-fluid">
    <div class="row">
        
        <main role="main" class="col-md-9 ml-sm-auto col-lg-10 pt-3 px-4">
            <h1>操作语义</h1>
<p>本文档介绍了在 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder</code></a> 接口中定义的操作语义。通常来说，这些操作与 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/xla_data.proto"><code>xla_data.proto</code></a> 中 RPC 接口所定义的操作是一一对应的。</p>
<p>关于术语：广义数据类型 XLA 处理的是一个 N - 维数组，其元素均为某种数据类型（如 32 位浮点数）。在本文档中，<strong>数组</strong> 表示任意维度的数组。为方便起见，有些特例使用人们约定俗成的更具体和更熟悉的名称；比如，1 维数组称为<strong>向量</strong>，2 维数组称为<strong>矩阵</strong>。</p>
<h2>BatchNormGrad</h2>
<p>算法详情参见 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::BatchNormGrad</code></a> 和 <a href="https://arxiv.org/abs/1502.03167">batch normalization 原始论文</a>。</p>
<p>计算 batch norm 的梯度</p>
<p><b> `BatchNormGrad(operand, scale, mean, variance, grad_output, epsilon, feature_index)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>类型</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>待归一化的 n 维数组 （x）</td>
</tr>
<tr>
<td><code>scale</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组 (\(\gamma\))</td>
</tr>
<tr>
<td><code>mean</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组 (\(\mu\))</td>
</tr>
<tr>
<td><code>variance</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组 (\(\sigma^2\))</td>
</tr>
<tr>
<td><code>grad_output</code></td>
<td><code>ComputationDataHandle</code></td>
<td>传入 <code>BatchNormTraining</code> 的梯度(\( \nabla y\))</td>
</tr>
<tr>
<td><code>epsilon</code></td>
<td><code>float</code></td>
<td>ε 值 (\(\epsilon\))</td>
</tr>
<tr>
<td><code>feature_index</code></td>
<td><code>int64</code></td>
<td><code>operand</code> 中的特征维数索引</td>
</tr>
</tbody>
</table></div>
<p>对于特征维数中的每一个特征（<code>feature_index</code> 即 <code>operand</code> 中特征维度的索引），此操作计算 <code>operand</code> 的梯度、在所有其他维度上的 <code>offset</code> 和 <code>scale</code>。<code>feature_index</code> 必须是 <code>operand</code> 中特征维度的合法索引。</p>
<p>[需要翻译]：The three gradients are defined by the following formulas (Assuming a 4-dimensional tensor as <code>operand</code> and (l) is the index for feature dimension):</p>
<p>\( coef<em>l = \frac{1}{mwh}\sum</em>{i=1}^m\sum<em>{j=1}^w\sum</em>{k=1}^h (\nabla y<em>{ijkl} * (x</em>{ijkl} - \mu<em>l) / (\sigma^2</em>{l}+\epsilon)) \)</p>
<p>\( \nabla x<em>{ijkl} = \gamma</em>{l} <em> (1/\sqrt{\sigma^2_{l}+\epsilon}) </em> [\nabla y<em>{ijkl} - mean(\nabla y) - (x</em>{ijkl} - \mu_{l}) * coef_l] \)</p>
<p>\( \nabla \beta<em>l = \sum</em>{i=1}^m\sum<em>{j=1}^w\sum</em>{k=1}^h \nabla y_{ijkl} \)</p>
<p>\( \nabla \gamma<em>l = \sum</em>{i=1}^m\sum<em>{j=1}^w\sum</em>{k=1}^h \nabla y<em>{ijkl} * ((x</em>{ijkl} - \mu<em>l) / \sqrt{\sigma^2</em>{l}+\epsilon}) \)</p>
<p>输入 <code>mean</code> 和 <code>variance</code> 表示在批处理和空间维度上的矩值。</p>
<p>输出类型是包含三个句柄的元组：</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>输出</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>grad_operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>输入 <code>operand</code> 的梯度 (\( \nabla x\))</td>
</tr>
<tr>
<td><code>grad_scale</code></td>
<td><code>ComputationDataHandle</code></td>
<td>输入 <code>scale</code> 的梯度 (\( \nabla \gamma\))</td>
</tr>
<tr>
<td><code>grad_offset</code></td>
<td><code>ComputationDataHandle</code></td>
<td>输入 <code>offset</code> 的梯度 (\( \nabla \beta\))</td>
</tr>
</tbody>
</table></div>
<h2>BatchNormInference</h2>
<p>算法详情参见 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::BatchNormInference</code></a> 和 <a href="https://arxiv.org/abs/1502.03167">batch normalization 原始论文</a>。</p>
<p>在批处理和空间维度上归一化数组。</p>
<p><b> `BatchNormInference(operand, scale, offset, mean, variance, epsilon, feature_index)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>待归一化的 n 维数组</td>
</tr>
<tr>
<td><code>scale</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组</td>
</tr>
<tr>
<td><code>offset</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组</td>
</tr>
<tr>
<td><code>mean</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组</td>
</tr>
<tr>
<td><code>variance</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组</td>
</tr>
<tr>
<td><code>epsilon</code></td>
<td><code>float</code></td>
<td>ε 值</td>
</tr>
<tr>
<td><code>feature_index</code></td>
<td><code>int64</code></td>
<td><code>operand</code> 中的特征维数索引</td>
</tr>
</tbody>
</table></div>
<p>对于特征维数中的每一个特征（<code>feature_index</code> 即 <code>operand</code> 中特征维度的索引），此操作计算在所有其他维度上的均值和方差，以及使用均值和方差归一化 <code>operand</code> 中的每个元素。<code>feature_index</code> 必须是 <code>operand</code> 中特征维度的合法索引。</p>
<p><code>BatchNormInference</code> 等价于在每批次中不计算 <code>mean</code> 和 <code>variance</code> 的情况下调用 <code>BatchNormTraining</code>。它使用 <code>mean</code> 和 <code>variance</code> 作为估计值。此操作的目的是减少推断中的延迟，因此命名为 <code>BatchNormInference</code>。</p>
<p>输出是一个 N 维的标准化数组，与输入 <code>operand</code> 的形状相同。</p>
<h2>BatchNormTraining</h2>
<p>算法详情参见 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::BatchNormTraining</code></a> 和 <a href="https://arxiv.org/abs/1502.03167"><code>batch normalization 原始论文</code></a>。</p>
<p>在批处理和空间维度上归一化数组。</p>
<p><b> `BatchNormTraining(operand, scale, offset, epsilon, feature_index)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>待归一化的 N 维数组 normalized (x)</td>
</tr>
<tr>
<td><code>scale</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组 (\(\gamma\))</td>
</tr>
<tr>
<td><code>offset</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组 (\(\beta\))</td>
</tr>
<tr>
<td><code>epsilon</code></td>
<td><code>float</code></td>
<td>Epsilon 值 (\(\epsilon\))</td>
</tr>
<tr>
<td><code>feature_index</code></td>
<td><code>int64</code></td>
<td><code>operand</code> 中的特征维数索引</td>
</tr>
</tbody>
</table></div>
<p>对于特征维数中的每一个特征（<code>feature_index</code> 即 <code>operand</code> 中特征维度的索引），此操作计算在所有其他维度上的均值和方差，以及使用均值和方差归一化 <code>operand</code> 中的每个元素。<code>feature_index</code> 必须是 <code>operand</code> 中特征维度的合法索引。</p>
<p>该算法对 <code>operand</code> \(x\) 中的每批次数据（包含 <code>w</code> 和 <code>h</code> 的 <code>m</code> 元素作为空间维度的大小）按如下次序执行：</p>
<ul>
<li><p>在特征维度中，对每个特征 <code>l</code> 计算批处理均值 \(\mu_l\):<br>
\(\mu<em>l=\frac{1}{mwh}\sum</em>{i=1}^m\sum<em>{j=1}^w\sum</em>{k=1}^h x_{ijkl}\)</p>
</li>
<li><p>计算批处理方差 \(\sigma^2_l\):<br>
\(\sigma^2<em>l=\frac{1}{mwh}\sum</em>{i=1}^m\sum<em>{j=1}^w\sum</em>{k=1}^h (x_{ijkl} - \mu_l)^2\)</p>
</li>
<li><p>归一化、缩放和平移:<br>
\(y_{ijkl}=\frac{\gamma<em>l(x</em>{ijkl}-\mu_l)}{\sqrt[2]{\sigma^2_l+\epsilon}}+\beta_l\)</p>
</li>
</ul>
<p>ε 值，通常是一个很小的数字，以避免 divide-by-zero 错误</p>
<p>输出类型是一个包含三个 <code>ComputationDataHandle</code> 类型元素的元组：</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>输出</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>output</code></td>
<td><code>ComputationDataHandle</code></td>
<td>与输入 <code>operand</code> (y)具有相同形状的 N 维数组</td>
</tr>
<tr>
<td><code>batch_mean</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组 (\(\mu\))</td>
</tr>
<tr>
<td><code>batch_var</code></td>
<td><code>ComputationDataHandle</code></td>
<td>1 维数组 (\(\sigma^2\))</td>
</tr>
</tbody>
</table></div>
<p>输入 <code>batch_mean</code> 和 <code>batch_var</code> 表示使用上述公式在批处理和空间维度上计算的矩值。</p>
<h2>BitcastConvertType</h2>
<p>同样参见<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::BitcastConvertType</code></a>.</p>
<p>类似于 TensorFlow 中的 <code>tf.bitcast</code>，对输入数据的每个元素进行 bitcast 操作，从而转化为目标形状。维度必须匹配，且转换是一对一的；如 <code>s32</code> 元素通过 bitcast 操作转化为 <code>f32</code>。Bitcast 采用底层 cast 操作，所以不同浮点数表示法的机器会产生不同的结果。</p>
<p><b> `BitcastConvertType(operand, new_element_type)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>D 维，类型为 T 的数组</td>
</tr>
<tr>
<td><code>new_element_type</code></td>
<td><code>PrimitiveType</code></td>
<td>类型 U</td>
</tr>
</tbody>
</table></div>
<p>operand 和 目标形状的维度必须匹配。源和目标元素类型的位宽必须一致。源和目标元素类型不能是元组。</p>
<h2>广播（Broadcast）</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Broadcast</code></a>。</p>
<p>通过在数组中复制数据来增加其维度。</p>
<p><b> `Broadcast(operand, broadcast_sizes)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数        </th>
<th>类型                  </th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code>        </td>
<td><code>ComputationDataHandle</code></td>
<td>待复制的数组</td>
</tr>
<tr>
<td><code>broadcast_sizes</code></td>
<td><code>ArraySlice&lt;int64&gt;</code>    </td>
<td>新维度的形状大小</td>
</tr>
</tbody>
</table></div>
<p>新的维度被插入在操作数（operand）的左侧，即，若 <code>broadcast_sizes</code> 的值为 <code>{a0, ..., aN}</code>，而操作数（operand）的维度形状为 <code>{b0, ..., bM}</code>，则广播后输出的维度形状为 <code>{a0, ..., aN, b0, ..., bM}</code>。</p>
<p>新的维度指标被插入到操作数（operand）副本中，即</p>
<pre><code>output[i0, ..., iN, j0, ..., jM] = operand[j0, ..., jM]
</code></pre>
<p>比如，若 <code>operand</code> 为一个值为 <code>2.0f</code> 的标量，且 <code>broadcast_sizes</code> 为 <code>{2, 3}</code>，则结果形状为 <code>f32[2, 3]</code> 的一个数组，且它的所有元素的值都为 <code>2.0f</code>。</p>
<h2>调用（Call）</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Call</code></a>。</p>
<p>给定参数情况下，触发计算。</p>
<p><b> `Call(computation, args...)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数    </th>
<th>类型                    </th>
<th>语义                      </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>computation</code></td>
<td><code>Computation</code>          </td>
<td>类型为 <code>T_0, T_1, ..., T_N -&gt;S</code> 的计算，它有 N 个任意类型的参数 </td>
</tr>
<tr>
<td><code>args</code>      </td>
<td>N 个 <code>ComputationDataHandle</code> 的序列          </td>
<td>任意类型的 N 个 参数</td>
</tr>
</tbody>
</table></div>
<p>参数 <code>args</code> 的数目和类型必须与计算 <code>computation</code> 相匹配。当然，没有参数 <code>args</code> 也是允许的。</p>
<h2>钳制（Clamp）</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Clamp</code></a>。</p>
<p>将一个操作数钳制在最小值和最大值之间的范围内。</p>
<p><b> `Clamp(min, operand, max)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数    </th>
<th>类型                  </th>
<th>语义                      </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>min</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
<tr>
<td><code>operand</code>    </td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
<tr>
<td><code>max</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
</tbody>
</table></div>
<p>给定操作数，最小和最大值，如果操作数位于最小值和最大值之间，则返回操作数，否则，如果操作数小于最小值，则返回最小值，如果操作数大于最大值，则返回最大值。即 <code>clamp(a, x, b) =  min(max(a, x), b)</code>。</p>
<p>输入的三个数组的维度形状必须是一样的。另外，也可以采用一种严格的<a href="broadcasting.md">广播</a>形式，即 <code>min</code> 和/或 <code>max</code> 可以是类型为 <code>T</code> 的一个标量。</p>
<p><code>min</code> 和 <code>max</code> 为标量的示例如下：</p>
<pre><code>let operand: s32[3] = {-1, 5, 9};
let min: s32 = 0;
let max: s32 = 6;
==&gt;
Clamp(min, operand, max) = s32[3]{0, 5, 6};
</code></pre>
<h2>折叠（Collapse）</h2>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/reshape"><code>tf.reshape</code></a></p>
<p>将一个数组的多个维度折叠为一个维度。</p>
<p><b> `Collapse(operand, dimensions)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数  </th>
<th>类型                  </th>
<th>语义                          </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code>  </td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组  </td>
</tr>
<tr>
<td><code>dimensions</code></td>
<td><code>int64</code> 矢量        </td>
<td>T 的维度形状的依次连续子集</td>
</tr>
</tbody>
</table></div>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/reshape"><code>tf.reshape</code></a></p>
<p>比如，令 v 为包含 24 个元素的数组：</p>
<pre><code>let v = f32[4x2x3] {{{10, 11, 12},  {15, 16, 17}},
                    {{20, 21, 22},  {25, 26, 27}},
                    {{30, 31, 32},  {35, 36, 37}},
                    {{40, 41, 42},  {45, 46, 47}}};

// 折叠至一个维度，即只留下一个维度
let v012 = Collapse(v, {0,1,2});
then v012 == f32[24] {10, 11, 12, 15, 16, 17,
                      20, 21, 22, 25, 26, 27,
                      30, 31, 32, 35, 36, 37,
                      40, 41, 42, 45, 46, 47};

// 折叠两个较低维度，剩下两个维度
let v01 = Collapse(v, {0,1});
then v01 == f32[4x6] {{10, 11, 12, 15, 16, 17},
                      {20, 21, 22, 25, 26, 27},
                      {30, 31, 32, 35, 36, 37},
                      {40, 41, 42, 45, 46, 47}};

// 折叠两个较高维度，剩下两个维度
let v12 = Collapse(v, {1,2});
then v12 == f32[8x3] {{10, 11, 12},
                      {15, 16, 17},
                      {20, 21, 22},
                      {25, 26, 27},
                      {30, 31, 32},
                      {35, 36, 37},
                      {40, 41, 42},
                      {45, 46, 47}};
</code></pre>
<h2>串连（Concatenate）</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::ConcatInDim</code></a>。</p>
<p>串连操作是将多个数组操作数合并成一个数组。输出数组与输入数组的秩必须是一样的（即要求输入数组的秩也要相同），并且它按输入次序包含了输入数组的所有元素。</p>
<p><b> `Concatenate(operands..., dimension)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operands</code></td>
<td>N 个 <code>ComputationDataHandle</code> 的序列</td>
<td>类型为 T 维度为 [L0, L1, ...] 的 N 个数组。要求 N&gt;=1</td>
</tr>
<tr>
<td><code>dimension</code></td>
<td><code>int64</code></td>
<td>区间 <code>[0, N)</code> 中的一个整数值，令那些 <code>operands</code> 能够串连起来的维度名</td>
</tr>
</tbody>
</table></div>
<p>除了 <code>dimension</code> 之外，其它维度都必须是一样的。这是因为 XLA 不支持 "不规则" 数组。还要注意的是，0-阶的标量值是无法串连在一起的（因为无法确定串连到底发生在哪个维度）。</p>
<p>1-维示例：</p>
<pre><code>Concat({{2, 3}, {4, 5}, {6, 7}}, 0)
&gt;&gt;&gt; {2, 3, 4, 5, 6, 7}
</code></pre>
<p>2-维示例：</p>
<pre><code>let a = {
  {1, 2},
  {3, 4},
  {5, 6},
};
let b = {
  {7, 8},
};
Concat({a, b}, 0)
&gt;&gt;&gt; {
  {1, 2},
  {3, 4},
  {5, 6},
  {7, 8},
}
</code></pre>
<p>图表：</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="https://www.tensorflow.org/images/ops_concatenate.png">
</div><h2>Conditional</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Conditional</code></a>.</p>
<p><b> `Conditional(pred, true_operand, true_computation, false_operand, false_computation)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>pred</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 <code>PRED</code> 的标量</td>
</tr>
<tr>
<td><code>true_operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 <code>T_0</code> 的参数</td>
</tr>
<tr>
<td><code>true_computation</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T_0 -&gt; S</code> 的计算</td>
</tr>
<tr>
<td><code>false_operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 <code>T_1</code> 的参数</td>
</tr>
<tr>
<td><code>false_computation</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T_0 -&gt; S</code> 的计算</td>
</tr>
</tbody>
</table></div>
<p>如果 <code>pred</code> 为 <code>true</code>，执行 <code>true_computation</code>，如果 <code>pred</code> 为 <code>false</code>，则返回结果。</p>
<p><code>true_computation</code> 必须接受一个类型为 <code>T_0</code> 的单参数，并使用 <code>true_operand</code> 来调用，它们必须类型相同。  <code>false_computation</code> 必须接受一个类型为 <code>T_1</code> 的单参数，并使用 <code>false_operand</code> 来调用，它们必须类型相同。 <code>true_computation</code> 和 <code>false_computation</code> 的返回值的类型必须相同。</p>
<p>注意，根据 <code>pred</code> 的值，<code>true_computation</code> 和 <code>false_computation</code> 只能执行其中一个。</p>
<h2>Conv (卷积)</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Conv</code></a>。</p>
<p>类似于 ConvWithGeneralPadding，但是边缘填充（padding）方式比较简单，要么是 SAME 要么是 VALID。SAME 方式将对输入（<code>lhs</code>）边缘填充零，使得在不考虑步长（striding）的情况下输出与输入的维度形状一致。VALID 填充方式则表示没有填充。</p>
<h2>ConvWithGeneralPadding (卷积)</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::ConvWithGeneralPadding</code></a>。</p>
<p>计算神经网络中使用的卷积。此处，一个卷积可被认为是一个 n-维窗口在一个 n-维底空间上移动，并对窗口的每个可能的位置执行一次计算。</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>秩为 n+2 的输入数组</td>
</tr>
<tr>
<td><code>rhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>秩为 n+2 的内核权重数组</td>
</tr>
<tr>
<td><code>window_strides</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>n-维内核步长数组</td>
</tr>
<tr>
<td><code>padding</code></td>
<td><code>ArraySlice&lt;pair&lt;int64, int64&gt;&gt;</code></td>
<td>n-维 (低, 高) 填充数据</td>
</tr>
<tr>
<td><code>lhs_dilation</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>n-维左边扩张因子数组</td>
</tr>
<tr>
<td><code>rhs_dilation</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>n-维右边扩张因子数组</td>
</tr>
</tbody>
</table></div>
<p>设 n 为空间维数。<code>lhs</code> 参数是一个 n+2 阶数组，它描述底空间区域的维度。它被称为输入，其实 rhs 也是输入。在神经网络中，它们都属于输入激励。n+2 维的含义依次为：</p>
<ul>
<li><code>batch</code>: 此维中每个坐标表示执行卷积的一个独立输入</li>
<li><code>z/depth/features</code>: 基空间区域中的每个 (y,x) 位置都指定有一个矢量，由这个维度来表示</li>
<li><code>spatial_dims</code>: 描述了定义了底空间区域的那 <code>n</code> 个空间维度，窗口要在它上面移动</li>
</ul>
<p><code>rhs</code> 参数是一个 n+2 阶的数组，它描述了卷积过滤器/内核/窗口。这些维度的含义依次为：</p>
<ul>
<li><code>output-z</code>: 输出的 <code>z</code> 维度。</li>
<li><code>input-z</code>: 此维度的大小等于 lhs 参数的 <code>z</code> 维度的大小。</li>
<li><code>spatial_dims</code>: 描述了定义此 n-维窗口的那 <code>n</code> 个空间维度，此窗口用于在底空间上移动。</li>
</ul>
<p><code>window_strides</code> 参数指定了卷积窗口在空间维度上的步长。比如，如果步长为 3，则窗口只用放在第一个空间维度指标为 3 的倍数的那些位置上。</p>
<p><code>padding</code> 参数指定了在底空间区域边缘填充多少个零。填充数目可以是负值 -- 这时数目绝对值表示执行卷积前要移除多少个元素。<code>padding[0]</code> 指定维度 <code>y</code> 的填充对子，<code>padding[1]</code> 指定的是维度 <code>x</code> 的填充对子。每个填充对子包含两个值，第一个值指定低位填充数目，第二个值指定高位填充数目。低位填充指的是低指标方向的填充，高位填充则是高指标方向的填充。比如，如果 <code>padding[1]</code> 为 <code>(2,3)</code>，则在第二个空间维度上，左边填充 2 个零，右边填充 3 个零。填充等价于在执行卷积前在输入 (<code>lhs</code>) 中插入这些零值。</p>
<p><code>lhs_dilation</code> 和 <code>rhs_dilation</code> 参数指定了扩张系数，分别应用于 lhs 和 rhs 的每个空间维度上。如果在一个空间维度上的扩张系数为 d，则 d-1 个洞将被插入到这个维度的每一项之间，从而增加数组的大小。这些洞被填充上 no-op 值，对于卷积来说表示零值。</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose"><code>tf.nn.conv2d_transpose</code></a></p>
<p>输出形状的维度含义依次为：</p>
<ul>
<li><code>batch</code>: 和输入（<code>lhs</code>）具有相同的 <code>batch</code> 大小。</li>
<li><code>z</code>: 和内核（<code>rhs</code>）具有相同的 <code>output-z</code> 大小。</li>
<li><code>spatial_dims</code>: 每个卷积窗口的有效位置的值。</li>
</ul>
<p>卷积窗口的有效位置是由步长和填充后的底空间区域大小所决定的。</p>
<p>为描述卷积到底干了什么，考虑一个二维卷积，为输出选择某个固定的 <code>batch</code>，<code>z</code>，<code>y</code>，<code>x</code> 坐标。则 <code>(y,x)</code> 是底空间区域中的某个窗口的一个角的位置（比如左上角，具体是哪个要看你如何编码其空间维度）。现在，我们从底空间区域中得到了一个二维窗口，其中每一个二维点都指定有一个一维矢量，所以，我们得到一个三维盒子。对于卷积过滤器而言，因为我们固定了输出坐标 <code>z</code>，我们也有一个三维盒子。这两个盒子具有相同的维度，所以我们可以让它们逐个元素地相乘并相加（类似于点乘）。最后得到输出值。</p>
<p>注意，如果 <code>output-z</code> 等于一个数，比如 5，则此窗口的每个位置都在输出的 <code>z</code> 维 上产生 5 个值。这些值对应于卷积过滤器的不同部分，即每个 <code>output-z</code> 坐标，都由一个独立的三维盒子生成。所以，你可以将其想象成 5 个分立的卷积，每个都用了不同的过滤器。</p>
<p>下面是一个考虑了填充和步长的二维卷积伪代码：</p>
<pre><code>for (b, oz, oy, ox) {  // 输出坐标
  value = 0;
  for (iz, ky, kx) {  // 内核坐标和输入 z
    iy = oy*stride_y + ky - pad_low_y;
    ix = ox*stride_x + kx - pad_low_x;
    if (底空间区域内的(iy, ix)是不在填充位置上的) {
      value += input(b, iz, iy, ix) * kernel(oz, iz, ky, kx);
    }
  }
  output(b, oz, oy, ox) = value;
}
</code></pre>
<h2>ConvertElementType</h2>
<p>另请参阅<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::ConvertElementType</code></a>.</p>
<p>与 C++ 中逐元素的 <code>static_cast</code> 类似，对输入数据的每个元素进行转换操作，从而转化为目标形状。维度必须匹配，且转换是一对一的；如 <code>s32</code> 元素通过 <code>s32</code>-to-<code>f32</code> 转换过程转换为 <code>f32</code>。</p>
<p><b> `ConvertElementType(operand, new_element_type)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>D 维类型为 T 的数组</td>
</tr>
<tr>
<td><code>new_element_type</code></td>
<td><code>PrimitiveType</code></td>
<td>类型 U</td>
</tr>
</tbody>
</table></div>
<p>操作数和目标形状的维度必须匹配。源和目标元素类型不能是元组。</p>
<p>一个 <code>T=s32</code> 到 <code>U=f32</code> 的转换将执行标准化的 int-to-float 转化过程，如 round-to-nearest-even。</p>
<blockquote><p>注意：目前没有指定精确的 float-to-int 和 visa-versa 转换，但是将来可能作为转换操作的附加参数。不是所有的目标都实现了所有可能的转换。</p>
</blockquote>
<pre><code>let a: s32[3] = {0, 1, 2};
let b: f32[3] = convert(a, f32);
then b == f32[3]{0.0, 1.0, 2.0}
</code></pre>
<h2>CrossReplicaSum</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::CrossReplicaSum</code></a>。</p>
<p>跨多个副本（replica）的求和。</p>
<p><b> `CrossReplicaSum(operand)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>跨多个副本待求和的数组。</td>
</tr>
</tbody>
</table></div>
<p>输出的维度形状与输入形状一样。比如，如果有两个副本，而操作数在这两个副本上的值分别为 <code>(1.0, 2.5)</code> 和 <code>(3.0, 5.25)</code>，则此操作在两个副本上的输出值都是 <code>(4.0, 7.75)</code>。</p>
<p>计算 CrossReplicaSum 的结果需要从每个副本中获得一个输入，所以，如果一个副本执行一个 CrossReplicaSum 结点的次数多于其它副本，则前一个副本将永久等待。因此这些副本都运行的是同一个程序，这种情况发生的机会并不多，其中一种可能的情况是，一个 while 循环的条件依赖于输入的数据，而被输入的数据导致此循环在一个副本上执行的次数多于其它副本。</p>
<h2>CustomCall</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::CustomCall</code></a>。</p>
<p>在计算中调用由用户提供的函数。</p>
<p><b> `CustomCall(target_name, args..., shape)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>target_name</code></td>
<td><code>string</code></td>
<td>函数名称。一个指向这个符号名称的调用指令会被发出</td>
</tr>
<tr>
<td><code>args</code></td>
<td>N 个 <code>ComputationDataHandle</code> 的序列</td>
<td>传递给此函数的 N 个任意类型的参数</td>
</tr>
<tr>
<td><code>shape</code></td>
<td><code>Shape</code></td>
<td>此函数的输出维度形状</td>
</tr>
</tbody>
</table></div>
<p>不管参数的数目和类型，此函数的签名（signature）都是一样的。</p>
<pre><code>extern "C" void target_name(void* out, void** in);
</code></pre>
<p>比如，如果使用 CustomCall 如下：</p>
<pre><code>let x = f32[2] {1,2};
let y = f32[2x3] {{10, 20, 30}, {40, 50, 60}};

CustomCall("myfunc", {x, y}, f32[3x3])
</code></pre>
<p><code>myfunc</code> 实现的一个示例如下：</p>
<pre><code>extern "C" void myfunc(void* out, void** in) {
  float (&amp;x)[2] = *static_cast&lt;float(*)[2]&gt;(in[0]);
  float (&amp;y)[2][3] = *static_cast&lt;float(*)[2][3]&gt;(in[1]);
  EXPECT_EQ(1, x[0]);
  EXPECT_EQ(2, x[1]);
  EXPECT_EQ(10, y[0][0]);
  EXPECT_EQ(20, y[0][1]);
  EXPECT_EQ(30, y[0][2]);
  EXPECT_EQ(40, y[1][0]);
  EXPECT_EQ(50, y[1][1]);
  EXPECT_EQ(60, y[1][2]);
  float (&amp;z)[3][3] = *static_cast&lt;float(*)[3][3]&gt;(out);
  z[0][0] = x[1] + y[1][0];
  // ...
}
</code></pre>
<p>这个用户提供的函数不能有副作用，而且它的执行结果必须是确定的（即两次同样的调用不能有不同结果）。</p>
<blockquote><p>注：用户函数的黑箱特点限制了编译器的优化潜力。所以，尽量使用原生的 XLA 操作来表示你的计算；只有在迫不得已的情况下才使用 CustomCall。</p>
</blockquote>
<h2>点乘（Dot）</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Dot</code></a>。</p>
<p><b> `Dot(lhs, rhs)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义                                     </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
<tr>
<td><code>rhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
</tbody>
</table></div>
<p>此操作的具体语义由它的两个操作数的秩来决定：</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>输入</th>
<th>输出</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td>矢量 [n] <code>dot</code> 矢量 [n]</td>
<td>标量</td>
<td>矢量点乘</td>
</tr>
<tr>
<td>矩阵 [m x k] <code>dot</code> 矢量 [k]</td>
<td>矢量 [m]</td>
<td>矩阵矢量乘法</td>
</tr>
<tr>
<td>矩阵 [m x k] <code>dot</code> 矩阵 [k x n]</td>
<td>矩阵 [m x n]</td>
<td>矩阵矩阵乘法</td>
</tr>
</tbody>
</table></div>
<p>此操作执行的是 <code>lhs</code> 的最后一维与 <code>rhs</code> 的倒数第二维之间的乘法结果的求和。因而计算结果会导致维度的 "缩减"。<code>lhs</code> 和 <code>rhs</code> 缩减的维度必须具有相同的大小。在实际中，我们会用到矢量之间的点乘，矢量/矩阵点乘，以及矩阵间的乘法。</p>
<h2>DotGeneral</h2>
<p>另请参阅<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::DotGeneral</code></a>.</p>
<p><b> `DotGeneral(lhs, rhs, dimension_numbers)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
<tr>
<td><code>rhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
<tr>
<td><code>dimension_numbers</code></td>
<td><code>DotDimensionNumbers</code></td>
<td>类型为 T 的数组</td>
</tr>
</tbody>
</table></div>
<p>和点乘一样，但是对于 'lhs' 和 'rhs' 允许收缩和指定批处理维数。</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>DotDimensionNumbers 成员</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td>'lhs_contracting_dimensions'</td>
<td>repeated int64</td>
<td>'lhs' 转换维数</td>
</tr>
<tr>
<td>'rhs_contracting_dimensions'</td>
<td>repeated int64</td>
<td>'rhs' 转换维数</td>
</tr>
<tr>
<td>'lhs_batch_dimensions'</td>
<td>repeated int64</td>
<td>'lhs' 批处理维数</td>
</tr>
<tr>
<td>'rhs_batch_dimensions'</td>
<td>repeated int64</td>
<td>'rhs' 批处理维数</td>
</tr>
</tbody>
</table></div>
<p>DotGeneral 根据 'dimension_numbers' 指定的维数进行转换操作，然后计算点积和。</p>
<p>与 'lhs' 和 'rhs' 有关的转换维数不需要相同，但是在 'lhs_contracting_dimensions' 和 'rhs_contracting_dimensions' 数组必须按照相同的顺序列出，同时具有相同的维数大小。且需要同时与 'lhs' 和 'rhs' 在同一个维度上。</p>
<p>以转换维数为例：</p>
<pre><code>lhs = { {1.0, 2.0, 3.0},
        {4.0, 5.0, 6.0} }

rhs = { {1.0, 1.0, 1.0},
        {2.0, 2.0, 2.0} }

DotDimensionNumbers dnums;
dnums.add_lhs_contracting_dimensions(1);
dnums.add_rhs_contracting_dimensions(1);

DotGeneral(lhs, rhs, dnums) -&gt; { {6.0, 12.0},
                                 {15.0, 30.0} }
</code></pre>
<p>'lhs' 和 'rhs' 的批处理维数必须相同，在两个数组中必须以相同的顺序列出，同时维数大小必须相同。[需要翻译]and must be ordered before contracting and non-contracting/non-batch dimension numbers。</p>
<p>批处理维数的例子（批处理大小为 2，2x2 矩阵）：</p>
<pre><code>lhs = { { {1.0, 2.0},
          {3.0, 4.0} },
        { {5.0, 6.0},
          {7.0, 8.0} } }

rhs = { { {1.0, 0.0},
          {0.0, 1.0} },
        { {1.0, 0.0},
          {0.0, 1.0} } }

DotDimensionNumbers dnums;
dnums.add_lhs_contracting_dimensions(2);
dnums.add_rhs_contracting_dimensions(1);
dnums.add_lhs_batch_dimensions(0);
dnums.add_rhs_batch_dimensions(0);

DotGeneral(lhs, rhs, dnums) -&gt; { { {1.0, 2.0},
                                   {3.0, 4.0} },
                                 { {5.0, 6.0},
                                   {7.0, 8.0} } }
</code></pre>
<div class="table-wrapper"><table>
<thead><tr>
<th>Input</th>
<th>Output</th>
<th>Semantics</th>
</tr>
</thead>
<tbody>
<tr>
<td>[b0, m, k] <code>dot</code> [b0, k, n]</td>
<td>[b0, m, n]</td>
<td>batch matmul</td>
</tr>
<tr>
<td>[b0, b1, m, k] <code>dot</code> [b0, b1, k, n]</td>
<td>[b0, b1, m, n]</td>
<td>batch matmul</td>
</tr>
</tbody>
</table></div>
<p>[需要翻译]It follows that the resulting dimension number starts with the batch dimension, then the 'lhs' non-contracting/non-batch dimension, and finally the 'rhs' non-contracting/non-batch dimension.</p>
<h2>DynamicSlice</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::DynamicSlice</code></a>.</p>
<p>DynamicSlice从动态 <code>start_indices</code> 输入数组中提取子数组。<code>size_indices</code> 为每个维度的切片大小，它在每个维度上指定了切片范围：[start, start + size)。<code>start_indices</code> 的秩必须为 1，且维数大小等于 <code>operand</code> 的秩。</p>
<p>注意：当前实现未定义切片索引越界（错误的运行时生成的'start_indices'）的情况。</p>
<p><b> `DynamicSlice(operand, start_indices, size_indices)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的 N 维数组</td>
</tr>
<tr>
<td><code>start_indices</code></td>
<td><code>ComputationDataHandle</code></td>
<td>N 个整数组成的秩为 1 的数组，其中包含每个维度的起始切片索引。值必须大于等于0</td>
</tr>
<tr>
<td><code>size_indices</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>N 个整数组成的列表，其中包含每个维度的切片大小。值必须大于 0，且 start + size 必须小于等于维度大小，从而避免封装维数大小的模运算</td>
</tr>
</tbody>
</table></div>
<p>1 维示例如下：</p>
<pre><code>let a = {0.0, 1.0, 2.0, 3.0, 4.0}
let s = {2}

DynamicSlice(a, s, {2}) produces:
  {2.0, 3.0}
</code></pre>
<p>2 维示例如下：</p>
<pre><code>let b =
 { {0.0,  1.0,  2.0},
   {3.0,  4.0,  5.0},
   {6.0,  7.0,  8.0},
   {9.0, 10.0, 11.0} }
let s = {2, 1}

DynamicSlice(b, s, {2, 2}) produces:
  { { 7.0,  8.0},
    {10.0, 11.0} }
</code></pre>
<h2>DynamicUpdateSlice</h2>
<p>另请参见<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::DynamicUpdateSlice</code></a>.</p>
<p>DynamicUpdateSlice 是在输入数组 <code>operand</code> 上，通过切片 <code>update</code> 操作覆盖 <code>start_indices</code> 后生成的结果。<code>update</code> 的形状决定了更新后结果的子数组的形状。 <code>start_indices</code> 的秩必须为 1，且维数大小等于 <code>operand</code> 的秩。</p>
<p>注意：当前实现未定义切片索引越界（错误的运行时生成的'start_indices'）的情况。</p>
<p><b> `DynamicUpdateSlice(operand, update, start_indices)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的 N 维数组</td>
</tr>
<tr>
<td><code>update</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的包含切片更新的 N 维数组，每个维度的更新形状必须大于 0 ，且 start + update 必须小于维度大小，从而避免越界更新索引</td>
</tr>
<tr>
<td><code>start_indices</code></td>
<td><code>ComputationDataHandle</code></td>
<td>N 个整数组成的秩为 1 的数组，其中包含每个维度的起始切片索引。值必须大于等于0</td>
</tr>
</tbody>
</table></div>
<p>1 维示例如下：</p>
<pre><code>let a = {0.0, 1.0, 2.0, 3.0, 4.0}
let u = {5.0, 6.0}
let s = {2}

DynamicUpdateSlice(a, u, s) produces:
  {0.0, 1.0, 5.0, 6.0, 4.0}
</code></pre>
<p>2 维示例如下：</p>
<pre><code>let b =
 { {0.0,  1.0,  2.0},
   {3.0,  4.0,  5.0},
   {6.0,  7.0,  8.0},
   {9.0, 10.0, 11.0} }
let u =
 { {12.0,  13.0},
   {14.0,  15.0},
   {16.0,  17.0} }

let s = {1, 1}

DynamicUpdateSlice(b, u, s) produces:
 { {0.0,  1.0,  2.0},
   {3.0, 12.0, 13.0},
   {6.0, 14.0, 15.0},
   {9.0, 16.0, 17.0} }
</code></pre>
<h2>逐个元素的二元算术操作</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Add</code></a>。</p>
<p>XLA 支持多个逐个元素的二元算术操作。</p>
<p><b> `Op(lhs, rhs)` </b></p>
<p>其中 <code>Op</code> 可以是如下操作之一：<code>Add</code> (加法), <code>Sub</code> (减法), <code>Mul</code> (乘法), <code>Div</code> (除法), <code>Rem</code> (余数), <code>Max</code> (最大值), <code>Min</code> (最小值), <code>LogicalAnd</code> (逻辑且), 或 <code>LogicalOr</code> (逻辑或)。</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义                                     </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>左操作数：类型为 T 的数组</td>
</tr>
<tr>
<td><code>rhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>右操作数：类型为 T 的数组</td>
</tr>
</tbody>
</table></div>
<p><a href="//xitu.github.io/tensorflow-docs-web/performance/xla/broadcasting.html">广播语义</a></p>
<p>当 <code>Op</code> 为 <code>Rem</code> 时，结果的符号与被除数一致，而结果的绝对值总是小于除数的绝对值。</p>
<p>不过，还是可以用如下接口来支持不同秩操作数的广播：</p>
<p><b> `Op(lhs, rhs, broadcast_dimensions)` </b></p>
<p>其中 <code>Op</code> 的含义同上。这种接口用于具有不同秩的数组之间的算术操作（比如将一个矩阵与一个矢量相加）。</p>
<p><a href="//xitu.github.io/tensorflow-docs-web/performance/xla/broadcasting.html">广播语义</a></p>
<h2>逐个元素的比较操作</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Eq</code></a>。</p>
<p>XLA 还支持标准的逐个元素的二元比较操作。注意：当比较浮点类型时，遵循的是标准的 IEEE 754 浮点数语义。</p>
<p><b> `Op(lhs, rhs)` </b></p>
<p>其中 <code>Op</code> 可以是如下操作之一：<code>Eq</code> (相等), <code>Ne</code> (不等), <code>Ge</code> (大于或等于), <code>Gt</code> (大于), <code>Le</code> (小于或等于), <code>Lt</code> (小于)。</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义                                     </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>左操作数：类型为 T 的数组</td>
</tr>
<tr>
<td><code>rhs</code></td>
<td><code>ComputationDataHandle</code></td>
<td>右操作数：类型为 T 的数组</td>
</tr>
</tbody>
</table></div>
<p><a href="//xitu.github.io/tensorflow-docs-web/performance/xla/broadcasting.html">广播语义</a></p>
<p>要想用广播来比较不同秩的数组，需要用到如下接口：</p>
<p><b> `Op(lhs, rhs, broadcast_dimensions)` </b></p>
<p>其中 <code>Op</code> 含义同上。这种接口应该用于不同阶的数组之间的比较操作（比如将一个矩阵加到一个矢量上）。</p>
<p><a href="//xitu.github.io/tensorflow-docs-web/performance/xla/broadcasting.html">广播语义</a></p>
<h2>逐个元素的一元函数</h2>
<p>ComputationBuilder 支持下列逐个元素的一元函数：</p>
<p><b>`Abs(operand)`</b> 逐个元素的绝对值 <code>x -&gt; |x|</code>。</p>
<p><b>`Ceil(operand)`</b> 逐个元素的整数上界 <code>x -&gt; ⌈x⌉</code>。</p>
<p><b>`Cos(operand)`</b> 逐个元素的余弦 <code>x -&gt; cos(x)</code>。</p>
<p><b>`Exp(operand)`</b> 逐个元素的自然幂指数 <code>x -&gt; e^x</code>。</p>
<p><b>`Floor(operand)`</b> 逐个元素的整数下界 <code>x -&gt; ⌊x⌋</code>。</p>
<p><b>`IsFinite(operand)`</b> 测试 <code>operand</code> 的每个元素是否是有限的，即不是正无穷或负无穷，也不是 <code>NaN</code>。该操作返回一个 <code>PRED</code> 值的数组，维度形状与输入一致，数组中的元素当且仅当相应的输入是有限时为 <code>true</code>，否则为 <code>false</code>。</p>
<p><b>`Log(operand)`</b> 逐个元素的自然对数 <code>x -&gt; ln(x)</code>。</p>
<p><b>`LogicalNot(operand)`</b> 逐个元素的逻辑非 <code>x -&gt; !(x)</code>。</p>
<p><b>`Neg(operand)`</b> 逐个元素取负值 <code>x -&gt; -x</code>。</p>
<p><b>`Sign(operand)`</b> 逐个元素求符号 <code>x -&gt; sgn(x)</code>，其中</p>
<p>$$\text{sgn}(x) = \begin{cases} -1 &amp; x &lt; 0\ 0 &amp; x = 0\ 1 &amp; x &gt; 0 \end{cases}$$</p>
<p>它使用的是 <code>operand</code> 的元素类型的比较运算符。</p>
<p><b>`Tanh(operand)`</b> 逐个元素的双曲正切 <code>x -&gt; tanh(x)</code>。</p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义                                     </th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>函数的操作数</td>
</tr>
</tbody>
</table></div>
<p>该函数应用于 <code>operand</code> 数组的每个元素，从而形成具有相同形状的数组。它允许操作数为标量（秩 0 ）</p>
<h2>Gather[需要翻译]</h2>
<p>The XLA gather operation stitches together several slices (each slice at a potentially different runtime offset) of an input tensor into an output tensor.</p>
<h3>General Semantics</h3>
<p>See also <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Gather</code></a>. For a more intuitive description, see the "Informal Description" section below.</p>
<p><b> `gather(operand, gather_indices, output_window_dims, elided_window_dims, window_bounds, gather_dims_to_operand_dims)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>Arguments</th>
<th>Type</th>
<th>Semantics</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>The tensor we’re gathering from.</td>
</tr>
<tr>
<td><code>gather_indices</code></td>
<td><code>ComputationDataHandle</code></td>
<td>Tensor containing the starting indices of the slices we're we're stitching together into the output tensor.</td>
</tr>
<tr>
<td><code>index_vector_dim</code></td>
<td><code>int64</code></td>
<td>The dimension in <code>gather_indices</code> that contains the starting indices.</td>
</tr>
<tr>
<td><code>output_window_dims</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>The set of dimensions in the  output shape that are <em>window dimensions</em> (defined below). Not all window dimensions may be present in the output shape.</td>
</tr>
<tr>
<td><code>elided_window_dims</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>The set of <em>window dimensions</em> that are not present in the output shape. <code>window_bounds[i]</code> must be <code>1</code> for all <code>i</code> in <code>elided_window_dims</code>.</td>
</tr>
<tr>
<td><code>window_bounds</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td><code>window_bounds[i]</code> is the bounds for  window dimension <code>i</code>. This includes both the window dimensions that are explicitly part of the output shape (via <code>output_window_dims</code>) and the window dimensions that are elided (via <code>elided_window_dims</code>).</td>
</tr>
<tr>
<td><code>gather_dims_to_operand_dims</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>A dimension map (the array is interpreted as mapping <code>i</code> to <code>gather_dims_to_operand_dims[i]</code>)  from the gather indices in <code>gather_indices</code> to the operand index space.  It has to be one-to-one and total.</td>
</tr>
</tbody>
</table></div>
<p>For every index <code>Out</code> in the output tensor, we compute two things (more precisely described later):</p>
<ul>
<li><p>An index into <code>gather_indices.rank</code> - <code>1</code> dimensions of <code>gather_indices</code>, which gives us a starting index of a slice, <em>operand slice</em>, in the operand tensor.  These <code>gather_indices.rank</code> - <code>1</code> dimensions are all the dimensions in <code>gather_indices</code> except <code>index_vector_dim</code>.</p>
</li>
<li><p>A <em>window index</em> that has the same rank as the operand.  This index is composed of the values in <code>Out</code> at dimensions <code>output_window_dims</code>, embedded with zeroes according to <code>elided_window_dims</code>.</p>
</li>
</ul>
<p>The <em>window index</em> is the relative index of the element in <em>operand slice</em> that should be present in the output at index <code>Out</code>.</p>
<p>The output is a tensor of rank <code>output_window_dims.size</code> + <code>gather_indices.rank</code> - <code>1</code>.  Additionally, as a shorthand, we define <code>output_gather_dims</code> of type <code>ArraySlice&lt;int64&gt;</code> as the set of dimensions in the output shape but not in <code>output_window_dims</code>, in ascending order.  E.g. if the output tensor has rank <code>5</code>, <code>output_window_dims</code> is {<code>2</code>, <code>4</code>} then <code>output_gather_dims</code> is {<code>0</code>, <code>1</code>, <code>3</code>}</p>
<p>If <code>index_vector_dim</code> is equal to <code>gather_indices.rank</code> we implicitly consider <code>gather_indices</code> to have a trailing <code>1</code> dimension (i.e. if <code>gather_indices</code> was of shape <code>[6,7]</code> and <code>index_vector_dim</code> is <code>2</code> then we implicitly consider the shape of <code>gather_indices</code> to be <code>[6,7,1]</code>).</p>
<p>The bounds for the output tensor along dimension <code>i</code> is computed as follows:</p>
<ol>
<li>If <code>i</code> is present in <code>output_gather_dims</code> (i.e. is equal to <code>output_gather_dims[k]</code> for some <code>k</code>) then we pick the corresponding dimension bounds out of <code>gather_indices.shape</code>, skipping <code>index_vector_dim</code> (i.e. pick <code>gather_indices.shape.dims</code>[<code>k</code>] if <code>k</code> &lt; <code>index_vector_dim</code> and <code>gather_indices.shape.dims</code>[<code>k</code>+<code>1</code>] otherwise).</li>
<li>If <code>i</code> is present in <code>output_window_dims</code> (i.e. equal to <code>output_window_dims</code>[<code>k</code>] for some <code>k</code>) then we pick the corresponding bound out of <code>window_bounds</code> after accounting for <code>elided_window_dims</code> (i.e. we pick <code>adjusted_window_bounds</code>[<code>k</code>] where <code>adjusted_window_bounds</code> is <code>window_bounds</code> with the bounds at indices <code>elided_window_dims</code> removed).</li>
</ol>
<p>The operand index <code>In</code> corresponding to an output index <code>Out</code> is computed as follows:</p>
<ol>
<li>Let <code>G</code> = { <code>Out</code>[<code>k</code>] for <code>k</code> in <code>output_gather_dims</code> }.  Use <code>G</code> to slice out vector <code>S</code> such that <code>S</code>[<code>i</code>] = <code>gather_indices</code>[Combine(<code>G</code>, <code>i</code>)] where Combine(A, b) inserts b at position <code>index_vector_dim</code> into A. Note that this is well defined even if <code>G</code> is empty -- if <code>G</code> is empty then <code>S</code> = <code>gather_indices</code>.</li>
<li>Create an index, <code>S</code><sub>`in`</sub>, into <code>operand</code> using <code>S</code> by scattering <code>S</code> using the <code>gather_dims_to_operand_dims</code> map (<code>S</code><sub>`in`</sub> is the starting indices for <em>operand slice</em> mentioned above).  More precisely:<ol>
<li><code>S</code><sub>`in`</sub>[<code>gather_dims_to_operand_dims</code>[<code>k</code>]] = <code>S</code>[<code>k</code>] if <code>k</code> &lt; <code>gather_dims_to_operand_dims.size</code>.</li>
<li><code>S</code><sub>`in`</sub>[<code>_</code>] = <code>0</code> otherwise.</li>
</ol>
</li>
<li>Create an index <code>W</code><sub>`in`</sub> into <code>operand</code> by scattering the indices at the output window dimensions in <code>Out</code> according to the <code>elided_window_dims</code> set (<code>W</code><sub>`in`</sub> is the <em>window index</em> mentioned above).  More precisely:<ol>
<li><code>W</code><sub>`in`</sub>[<code>window_dims_to_operand_dims</code>(<code>k</code>)] = <code>Out</code>[<code>k</code>] if <code>k</code> &lt; <code>output_window_dims.size</code> (<code>window_dims_to_operand_dims</code> is defined below).</li>
<li><code>W</code><sub>`in`</sub>[<code>_</code>] = <code>0</code> otherwise.</li>
</ol>
</li>
<li><code>In</code> is <code>W</code><sub>`in`</sub> + <code>S</code><sub>`in`</sub> where + is element-wise addition.</li>
</ol>
<p><code>window_dims_to_operand_dims</code> is the monotonic function with domain [<code>0</code>, <code>output_window_dims.size</code>) and range [<code>0</code>, <code>operand.rank</code>) \ <code>elided_window_dims</code>.  So if, e.g., <code>output_window_dims.size</code> is <code>4</code>, <code>operand.rank</code> is <code>6</code> and <code>elided_window_dims</code> is {<code>0</code>, <code>2</code>} then <code>window_dims_to_operand_dims</code> is {<code>0</code>→<code>1</code>, <code>1</code>→<code>3</code>, <code>2</code>→<code>4</code>, <code>3</code>→<code>5</code>}.</p>
<h3>Informal Description and Examples</h3>
<p><code>index_vector_dim</code> is set to <code>gather_indices.rank</code> - <code>1</code> in all of the examples that follow.  More interesting values for <code>index_vector_dim</code> does not change the operation fundamentally, but makes the visual representation more cumbersome.</p>
<p>To get an intuition on how all of the above fits together, let's look at an example that gathers 5 slices of shape <code>[8,6]</code> from a <code>[16,11]</code> tensor.  The position of a slice into the <code>[16,11]</code> tensor can be represented as an index vector of shape <code>S64[2]</code>, so the set of 5 positions can be represented as a <code>S64[5,2]</code> tensor.</p>
<p>The behavior of the gather operation can then be depicted as an index transformation that takes [<code>G</code>,<code>W</code><sub>`0`</sub>,<code>W</code><sub>`1`</sub>], an index in the output shape, and maps it to an element in the input tensor in the following way:</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="../../images/ops_xla_gather_0.svg">
</div><p>We first select an (<code>X</code>,<code>Y</code>) vector from the gather indices tensor using <code>G</code>. The element in the output tensor at index [<code>G</code>,<code>W</code><sub>`0`</sub>,<code>W</code><sub>`1`</sub>] is then the element in the input tensor at index [<code>X</code>+<code>W</code><sub>`0`</sub>,<code>Y</code>+<code>W</code><sub>`1`</sub>].</p>
<p><code>window_bounds</code> is <code>[8,6]</code>, which decides the range of W<sub>`0`</sub> and W<sub>`1`</sub>, and this in turn decides the bounds of the slice.</p>
<p>This gather operation acts as a batch dynamic slice with <code>G</code> as the batch dimension.</p>
<p>The gather indices may be multidimensional.  For instance, a more general version of the example above using a "gather indices" tensor of shape <code>[4,5,2]</code><br>
would translate indices like this:</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="../../images/ops_xla_gather_1.svg">
</div><p>Again, this acts as a batch dynamic slice <code>G</code><sub>`0`</sub> and <code>G</code><sub>`1`</sub> as the batch dimensions.  The window bounds are still <code>[8,6]</code>.</p>
<p>The gather operation in XLA generalizes the informal semantics outlined above in the following ways:</p>
<ol>
<li><p>We can configure which dimensions in the output shape are the window dimensions (dimensions containing <code>W</code><sub>`0`</sub>, <code>W</code><sub>`1`</sub> in the last example).  The output gather dimensions (dimensions containing <code>G</code><sub>`0`</sub>, <code>G</code><sub>`1`</sub> in the last example) are defined to be the output dimensions that are not window dimensions.</p>
</li>
<li><p>The number of output window dimensions explicitly present in the output shape may be smaller than the input rank.  These "missing" dimensions, which are listed explicitly as <code>elided_window_dims</code>, must have a window bound of <code>1</code>.  Since they have a window bound of <code>1</code> the only valid index for them is <code>0</code> and eliding them does not introduce ambiguity.</p>
</li>
<li><p>The slice extracted from the "Gather Indices" tensor ((<code>X</code>, <code>Y</code>) in the last example) may have fewer elements than the input tensor rank, and an explicit mapping dictates how the index should be expanded to have the same rank as the input.</p>
</li>
</ol>
<p>As a final example, we use (2) and (3) to implement <code>tf.gather_nd</code>:</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="../../images/ops_xla_gather_2.svg">
</div><p><code>G</code><sub>`0`</sub> and <code>G</code><sub>`1`</sub> are used to slice out a starting index from the gather indices tensor as usual, except the starting index has only one element, <code>X</code>.  Similarly, there is only one output window index with the value <code>W</code><sub>`0`</sub>.  However, before being used as indices into the input tensor, these are expanded in accordance to "Gather Index Mapping" (<code>gather_dims_to_operand_dims</code> in the formal description) and "Window Mapping" (<code>window_dims_to_operand_dims</code> in the formal description) into [<code>0</code>,<code>W</code><sub>`0`</sub>] and [<code>X</code>,<code>0</code>] respectively, adding up to [<code>X</code>,<code>W</code><sub>`0`</sub>].  In other words, the output index [<code>G</code><sub>`0`</sub>,<code>G</code><sub>`1`</sub>,<code>W</code><sub>`0`</sub>] maps to the input index [<code>GatherIndices</code>[<code>G</code><sub>`0`</sub>,<code>G</code><sub>`1`</sub>,<code>0</code>],<code>X</code>] which gives us the semantics for <code>tf.gather_nd</code>.</p>
<p><code>window_bounds</code> for this case is <code>[1,11]</code>.  Intuitively this means that every index <code>X</code> in the gather indices tensor picks an entire row and the result is the concatenation of all these rows.</p>
<h2>GetTupleElement</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::GetTupleElement</code></a>。</p>
<p>将索引添加到编译时常量的元组中。</p>
<p>该值必须是编译时常量，这样才可以通过形状推断确定结果值的类型。</p>
<p>概念上，这类似于 C++ 中的 <code>std::get&lt;int N&gt;(t)</code>：</p>
<pre><code>let v: f32[10] = f32[10]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
let s: s32 = 5;
let t: (f32[10], s32) = tuple(v, s);
let element_1: s32 = gettupleelement(t, 1);  // 推断出的形状匹配 s32.
</code></pre>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/tuple"><code>tf.tuple</code></a></p>
<h2>Infeed</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Infeed</code></a>.</p>
<p><b> `Infeed(shape)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>shape</code></td>
<td><code>Shape</code></td>
<td>从 Infeed 接口读取数据的维度形状。此形状的数据布局必须与发送到设备上的数据相匹配；否则行为是未定义的</td>
</tr>
</tbody>
</table></div>
<p>从设备的隐式 Infeed 流接口读取单个数据项，根据给定的形状和布局来进行解析，并返回一个此数据的 <code>ComputationDataHandle</code>。在一个计算中允许有多个 Infeed 操作，但这些 Infeed 操作之间必须是全序的。比如，下面代码中两个 Infeed 是全序的，因为在不同 while 循环之间有依赖关系。</p>
<pre><code>result1 = while (condition, init = init_value) {
  Infeed(shape)
}

result2 = while (condition, init = result1) {
  Infeed(shape)
}
</code></pre>
<p>不支持嵌套的元组形状。对于一个空的元组形状，Infeed 操作通常是一个 no-op，因而不会从设备的 Infeed 中读取任何数据。</p>
<blockquote><p>注意：我们计划允许支持没有全序的多个 Infeed 操作，在这种情况下，编译器将提供信息，确定这些 Infeed 操作在编译后的程序中如何串行化。</p>
</blockquote>
<h2>映射（Map）</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Map</code></a>。</p>
<p><b> `Map(operands..., computation)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operands</code></td>
<td>N 个 <code>ComputationDataHandle</code> 的序列</td>
<td>类型为 T<em>0..T</em>{N-1} 的 N 个数组</td>
</tr>
<tr>
<td><code>computation</code></td>
<td><code>Computation</code></td>
<td>类型为<code>T_0, T_1, ..., T_{N + M -1} -&gt; S</code> 的计算，有 N 个类型为 T 的参数，和 M 个任意类型的参数</td>
</tr>
<tr>
<td><code>dimensions</code></td>
<td><code>int64</code> array</td>
<td>映射维度的数组</td>
</tr>
<tr>
<td><code>static_operands</code></td>
<td>M 个 <code>ComputationDataHandle</code> 的序列</td>
<td>任意类型的 M 个数组</td>
</tr>
</tbody>
</table></div>
<p>将一个标量函数作用于给定的 <code>operands</code> 数组，可产生相同维度的数组，其中每个元素都是映射函数（mapped function）作用于相应输入数组中相应元素的结果，而 <code>static_operands</code> 是 <code>computation</code> 的附加输入。</p>
<p>此映射函数可以是任意计算过程，只不过它必须有 N 个类型为 <code>T</code> 的标量参数，和单个类型为 <code>S</code> 的输出。输出的维度与输入 <code>operands</code> 相同，只不过元素类型 T 换成了 S。</p>
<p>比如，<code>Map(op1, op2, op3, computation, par1)</code> 用 <code>elem_out &lt;-
computation(elem1, elem2, elem3, par1)</code> 将输入数组中的每个（多维）指标映射产生输出数组。</p>
<h2>填充（Pad）</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Pad</code></a>。</p>
<p><b> `Pad(operand, padding_value, padding_config)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 <code>T</code> 的数组</td>
</tr>
<tr>
<td><code>padding_value</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 <code>T</code> 的标量，用于填充</td>
</tr>
<tr>
<td><code>padding_config</code></td>
<td><code>PaddingConfig</code></td>
<td>每个维度的两端的填充量 (low, high)</td>
</tr>
</tbody>
</table></div>
<p>通过在数组周围和数组之间进行填充，可以将给定的 <code>operand</code> 数组扩大，其中 <code>padding_value</code> 和 <code>padding_config</code> 用于配置每个维度的边缘填充和内部填充的数目。</p>
<p><code>PaddingConfig</code> 是 <code>PaddingConfigDimension</code> 的一个重复字段，它对于每个维度都包含有三个字段：<code>edge_padding_low</code>, <code>edge_padding_high</code> 和 <code>interior_padding</code>。<code>edge_padding_low</code> 和 <code>edge_padding_high</code> 分别指定了该维度上低端（指标为 0 那端）和高端（最高指标那端）上的填充数目。边缘填充数目可以是负值 — 负的填充数目的绝对值表示从指定维度移除元素的数目。<code>interior_padding</code> 指定了在每个维度的任意两个相邻元素之间的填充数目。逻辑上，内部填充应发生在边缘填充之前，所有在负边缘填充时，会从经过内部填充的操作数之上再移除边缘元素。如果边缘填充配置为 (0, 0)，且内部填充值都是 0，则此操作是一个 no-op。下图展示的是二维数组上不同 <code>edge_padding</code> 和 <code>interior_padding</code> 值的示例。</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="https://www.tensorflow.org/images/ops_pad.png">
</div><h2>Recv</h2>
<p>另请参阅<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Recv</code></a>.</p>
<p><b> `Recv(shape, channel_handle)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>shape</code></td>
<td><code>Shape</code></td>
<td>要接收的数据的形状</td>
</tr>
<tr>
<td><code>channel_handle</code></td>
<td><code>ChannelHandle</code></td>
<td>发送/接收对的唯一标识</td>
</tr>
</tbody>
</table></div>
<p>从另一台共享相同通道句柄的计算机的 <code>Send</code> 指令接收指定形状的数据，返回一个接收数据的 ComputationDataHandle。</p>
<p>客户端 <code>Recv</code> 操作的客户端 API 是同步通信。但是，指令内分解成 2 个 HLO 指令（<code>Recv</code> 和 <code>RecvDone</code>）用于异步数据传输。请参考 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/hlo_instruction.h"><code>HloInstruction::CreateRecv</code> 和 <code>HloInstruction::CreateRecvDone</code></a></p>
<p><b>`Recv(const Shape& shape, int64 channel_id)`</b></p>
<p>分配资源从具有相同 channel_id 的 <code>Send</code> 指令接收数据。返回已分配资源的上下文，该上下文随后通过 <code>RecvDone</code> 指令等待数据传输完成。上下文是 {接收缓冲区 (形状), 请求标识符（U32）} 的元组，且只能用于 <code>RecvDone</code> 指令。</p>
<p><b> `RecvDone(HloInstruction context)` </b></p>
<p>给定一个由 <code>Recv</code> 指令创建的上下文，等待数据传输完成并返回接收的数据。</p>
<h2>Reduce</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Reduce</code></a>。</p>
<p>将一个归约函数作用于一个数组。</p>
<p><b> `Reduce(operand, init_value, computation, dimensions)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 <code>T</code> 的数组</td>
</tr>
<tr>
<td><code>init_value</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 <code>T</code> 的标量</td>
</tr>
<tr>
<td><code>computation</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T, T -&gt; T</code>的计算</td>
</tr>
<tr>
<td><code>dimensions</code></td>
<td><code>int64</code> 数组</td>
<td>待归约的未排序的维度数组</td>
</tr>
</tbody>
</table></div>
<p>从概念上看，归约（Reduce）操作将输入数组中的一个或多个数组归约为标量。结果数组的秩为 <code>rank(operand) - len(dimensions)</code>。 <code>init_value</code> 是每次归约的初值，如果后端有需求也可以在计算中插入到任何地方。所以，在大多数情况下，<code>init_value</code> 应该为归约函数的一个单位元（比如，加法中的 0）。</p>
<p>归约函数的执行顺序是任意的，即可能是非确定的。因而，归约函数不应对运算的结合性敏感。</p>
<p>有些归约函数，比如加法，对于浮点数并没有严格遵守结合率。不过，如果数据的范围是有限的，则在大多数实际情况中，浮点加法已经足够满足结合率。当然，我们也可以构造出完全不遵守结合率的归约函数，这时，XLA 归约就会产生不正确或不可预测的结果。</p>
<p>下面是一个示例，对 1D 数组 [10, 11, 12, 13] 进行归约，归约函数为 <code>f</code> （即参数 <code>computation</code>），则计算结果为：</p>
<p><code>f(10, f(11, f(12, f(init_value, 13)))</code></p>
<p>但它还有其它很多种可能性，比如：</p>
<p><code>f(init_value, f(f(10, f(init_value, 11)), f(f(init_value, 12), f(13,
init_value))))</code></p>
<p>下面是一段实现归约的伪代码，归约计算为求和，初值为 0。</p>
<pre><code class="lang-python">result_shape &lt;- 从 operand_shape 的维度中移除所有待归约的维度

# 遍历 result_shape 中的所有元素，这里，r 的数目等于 result 的秩
for r0 in range(result_shape[0]), r1 in range(result_shape[1]), ...:
  # 初始化 result 的元素
  result[r0, r1...] &lt;- 0

  # 遍历所有的归约维度
  for d0 in range(dimensions[0]), d1 in range(dimensions[1]), ...:
    # 用 operand 的元素的值来增加 result 中的元素的值
    # operand 的元素的索引由所有的 ri 和 di 按正确的顺序构造而来
    # （构造得到的索引用来访问 operand 的整个形状）
    result[r0, r1...] += operand[ri... di]
</code></pre>
<p>下面是一个对 2D 数组（矩阵）进行归约的示例。其形状的秩为 2，0 维大小为 2，1 维大小为 3：</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/ops_2d_matrix.png">
</div><p>对 0 维或 1 维进行求和归约：</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/ops_reduce_from_2d_matrix.png">
</div><p>注意，两个归约结果都是一维数组。图中将一个显示为行，另一个显示为列，但这只是为了可视化效果。</p>
<p>下面是一个更复杂的 3D 数组的例子。它的秩为 3 ，形状为 (4,2,3)。为简单起见，我们让 1 到 6 这几个数字沿 0 维复制 4 份。</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/ops_reduce_from_3d_matrix.png">
</div><p>类似于二维的情况，我们可以只归约一个维度。如果我们归约第 0 维，我们得到一个二阶数组，它沿第 0 维的所有值会合并为一个标量：</p>
<pre><code class="lang-text">|  4   8  12 |
| 16  20  24 |
</code></pre>
<p>如果我们归约第 2 维，结果仍然是一个二阶数组，沿第 2 维的所有值合并为一个标量：</p>
<pre><code class="lang-text">| 6  15 |
| 6  15 |
| 6  15 |
| 6  15 |
</code></pre>
<p>注意，输出中剩下的维度的顺序与它们在输入中的相对顺序保持一致，只不过维度的名称（数字）会发生变化，因为数组的秩发生了变化。</p>
<p>我们也可以归约多个维度。对 0 维和 1 维进行求和归约，将得到一个一维数组 <code>| 20 28 36 |</code>。</p>
<p>对这个三维数组的所有元素进行求和归约，得到一个标量 <code>84</code>。</p>
<h2>ReducePrecision</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::ReducePrecision</code></a>。</p>
<p>当浮点数转换为低精度格式（比如 IEEE-FP16）然后转换回原格式时，值可能会发生变化，ReducePrecision 对这种变化进行建模。低精度格式中的指数（exponent）和尾数（mantissa）的位数目是可以任意指定的，不过不是所有硬件实现都支持所有的位大小。</p>
<p><b> `ReducePrecision(operand, mantissa_bits, exponent_bits)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>浮点类型 <code>T</code> 的数组</td>
</tr>
<tr>
<td><code>exponent_bits</code></td>
<td><code>int32</code></td>
<td>低精度格式中的指数位数</td>
</tr>
<tr>
<td><code>mantissa_bits</code></td>
<td><code>int32</code></td>
<td>低精度格式中的尾数位数</td>
</tr>
</tbody>
</table></div>
<p>结果为类型为 <code>T</code> 的数组。输入值被舍入至与给定尾数位的数字最接近的那个值（采用的是"偶数优先"原则）。而超过指数位所允许的值域时，输入值会被视为正无穷或负无穷。<code>NaN</code> 值会保留，不过它可能会被转换为规范化的 NaN 值。</p>
<p>低精度格式必须至少有一个指数位（为了区分零和无穷，因为两者的尾数位都为零），且尾数位必须是非负的。指数或尾数位可能会超过类型 <code>T</code>；这种情况下，相应部分的转换就仅仅是一个 no-op 了。</p>
<h2>ReduceWindow</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::ReduceWindow</code></a>。</p>
<p>将一个归约函数应用于输入多维数组的每个窗口内的所有元素上，输出一个多维数组，其元素个数等于合法窗口的元素数目。一个池化层可以表示为一个 <code>ReduceWindow</code>。</p>
<p><b> `ReduceWindow(operand, init_value, computation, window_dimensions,
window_strides, padding)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的 N 维数组。这是窗口放置的底空间区域</td>
</tr>
<tr>
<td><code>init_value</code></td>
<td><code>ComputationDataHandle</code></td>
<td>归约的初始值。细节请参见 <a href="#reduce">规约</a>。</td>
</tr>
<tr>
<td><code>computation</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T, T -&gt; T</code>的归约函数，应用于每个窗口内的所有元素</td>
</tr>
<tr>
<td><code>window_dimensions</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>表示窗口维度值的整数数组</td>
</tr>
<tr>
<td><code>window_strides</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>表示窗口步长值的整数数组</td>
</tr>
<tr>
<td><code>padding</code></td>
<td><code>Padding</code></td>
<td>窗口的边缘填充类型（Padding\:\:kSame 或 Padding\:\:kValid）</td>
</tr>
</tbody>
</table></div>
<p>下列代码和图为一个使用 <code>ReduceWindow</code> 的示例。输入是一个大小为 [4x6] 的矩阵，window_dimensions 和 window_stride_dimensions 都是 [2x3]。</p>
<pre><code>// 创建一个归约计算（求最大值）
Computation max;
{
  ComputationBuilder builder(client_, "max");
  auto y = builder.Parameter(0, ShapeUtil::MakeShape(F32, {}), "y");
  auto x = builder.Parameter(1, ShapeUtil::MakeShape(F32, {}), "x");
  builder.Max(y, x);
  max = builder.Build().ConsumeValueOrDie();
}

// 用最大值归约计算来创建一个 ReduceWindow 计算
ComputationBuilder builder(client_, "reduce_window_2x3");
auto shape = ShapeUtil::MakeShape(F32, {4, 6});
auto input = builder.Parameter(0, shape, "input");
builder.ReduceWindow(
    input, *max,
    /*init_val=*/builder.ConstantLiteral(LiteralUtil::MinValue(F32)),
    /*window_dimensions=*/{2, 3},
    /*window_stride_dimensions=*/{2, 3},
    Padding::kValid);
</code></pre>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:35%" src="https://www.tensorflow.org/images/ops_reduce_window.png">
</div><p>在维度中，步长为 1 表示在此维度上两个相邻窗口间隔一个元素，为了让窗口互相不重叠，window_stride_dimensions 和 window_dimensions 应该要相等。下图给出了两种不同步长设置的效果。边缘填充应用于输入的每个维度，计算过程实际发生在填充之后的数组上。</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:75%" src="https://www.tensorflow.org/images/ops_reduce_window_stride.png">
</div><p>归约函数的执行顺序是任意的，因而结果可能是非确定性的。所以，归约函数应该不能对计算的结合性太过敏感。更多细节，参见 <a href="#reduce"><code>Reduce</code></a> 关于结合性的讨论。</p>
<h2>Reshape</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Reshape</code></a> 和 <a href="#collapse"><code>Collapse</code></a> 操作。</p>
<p>变形操作（reshape）是将一个数组的维度变成另外一种维度设置。</p>
<p><b> `Reshape(operand, new_sizes)` </b><br>
<b> `Reshape(operand, dimensions, new_sizes)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
<tr>
<td><code>dimensions</code></td>
<td><code>int64</code> vector</td>
<td>维度折叠的顺序</td>
</tr>
<tr>
<td><code>new_sizes</code></td>
<td><code>int64</code> vector</td>
<td>新维度大小的矢量</td>
</tr>
</tbody>
</table></div>
<p>从概念上看，变形操作首先将一个数组拉平为一个一维矢量，然后将此矢量展开为一个新的形状。输入参数是一个类型为 T 的任意数组，一个编译时常量的维度指标数组，以及表示结果维度大小的一个编译时常量的数组。如果给出了 <code>dimensions</code> 参数，这个矢量中的值必须是 T 的所有维度的一个置换，其默认值为 <code>{0, ..., rank - 1}</code>。<code>dimensions</code> 中的维度的顺序是从最慢变化维（最主序）到最快变化维（最次序），按照这个顺序依次将所有元素折叠到一个维度上。<code>new_sizes</code> 矢量决定了输出数组的维度大小。<code>new_sizes[0]</code> 表示第 0 维的大小，<code>new_sizes[1]</code> 表示的是第 1 维的大小，依此类推。<code>new_sizes</code> 中的维度值的乘积必须等于 operand 的维度值的乘积。将折叠的一维数组展开为由 <code>new_sizes</code> 定义的多维数组时，<code>new_sizes</code> 中的维度的顺序也是最慢变化维（最主序）到最快变化维（最次序）。</p>
<p>比如，令 v 为包含 24 个元素的数组：</p>
<pre><code>let v = f32[4x2x3] {{{10, 11, 12}, {15, 16, 17}},
                    {{20, 21, 22}, {25, 26, 27}},
                    {{30, 31, 32}, {35, 36, 37}},
                    {{40, 41, 42}, {45, 46, 47}}};

依次折叠:
let v012_24 = Reshape(v, {0,1,2}, {24});
then v012_24 == f32[24] {10, 11, 12, 15, 16, 17, 20, 21, 22, 25, 26, 27,
                         30, 31, 32, 35, 36, 37, 40, 41, 42, 45, 46, 47};

let v012_83 = Reshape(v, {0,1,2}, {8,3});
then v012_83 == f32[8x3] {{10, 11, 12}, {15, 16, 17},
                          {20, 21, 22}, {25, 26, 27},
                          {30, 31, 32}, {35, 36, 37},
                          {40, 41, 42}, {45, 46, 47}};

乱序折叠:
let v021_24 = Reshape(v, {1,2,0}, {24});
then v012_24 == f32[24]  {10, 20, 30, 40, 11, 21, 31, 41, 12, 22, 32, 42,
                          15, 25, 35, 45, 16, 26, 36, 46, 17, 27, 37, 47};

let v021_83 = Reshape(v, {1,2,0}, {8,3});
then v021_83 == f32[8x3] {{10, 20, 30}, {40, 11, 21},
                          {31, 41, 12}, {22, 32, 42},
                          {15, 25, 35}, {45, 16, 26},
                          {36, 46, 17}, {27, 37, 47}};


let v021_262 = Reshape(v, {1,2,0}, {2,6,2});
then v021_262 == f32[2x6x2] {{{10, 20}, {30, 40},
                              {11, 21}, {31, 41},
                              {12, 22}, {32, 42}},
                             {{15, 25}, {35, 45},
                              {16, 26}, {36, 46},
                              {17, 27}, {37, 47}}};
</code></pre>
<p>作为特例，单元素数组和标量之间可以用变形操作相互转化。比如：</p>
<pre><code>Reshape(f32[1x1] {{5}}, {0,1}, {}) == 5;
Reshape(5, {}, {1,1}) == f32[1x1] {{5}};
</code></pre>
<h2>Rev (反转)</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Rev</code></a>。</p>
<p><b>`Rev(operand, dimensions)`</b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组 </td>
</tr>
<tr>
<td><code>dimensions</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>待反转的维度</td>
</tr>
</tbody>
</table></div>
<p>反转操作是将 <code>operand</code> 数组沿指定的维度 <code>dimensions</code> 对元素的顺序反转，产生一个形状相同的数组。operand 数组的每个元素被存储在输出数组的变换后的位置上。元素的原索引位置在每个待倒置维度上都被反转了，得到其在输出数组中的索引位置（即，如果一个大小为 N 的维度是待倒置的，则索引 i 被变换为 N-i-i）。</p>
<p><code>Rev</code> 操作的一个用途是在神经网络的梯度计算时沿两个窗口维度对卷积权重值进行倒置。</p>
<h2>RngNormal</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::RngNormal</code></a>。</p>
<p>RngNormal 构造一个符合 $$(\mu, \sigma)$$ 正态随机分布的指定形状的随机数组。参数 <code>mu</code> 和 <code>sigma</code> 为 F32 类型的标量值，而输出形状为 F32 的数组。</p>
<p><b>`RngNormal(mean, sigma, shape)`</b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>mu</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 F32 的标量，指定生成的数的均值</td>
</tr>
<tr>
<td><code>sigma</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 F32 的标量，指定生成的数的标准差</td>
</tr>
<tr>
<td><code>shape</code></td>
<td><code>Shape</code></td>
<td>类型为 F32 的输出的形状</td>
</tr>
</tbody>
</table></div>
<h2>RngUniform</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::RngUniform</code></a>。</p>
<p>RngNormal 构造一个符合区间 $$[a,b)$$ 上的均匀分布的指定形状的随机数组。参数和输出形状可以是 F32、S32 或 U32，但是类型必须是一致的。此外，参数必须是标量值。如果 $$b &lt;= a$$，输出结果与具体的实现有关。</p>
<p><b>`RngUniform(a, b, shape)`</b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>a</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的标量，指定区间的下界</td>
</tr>
<tr>
<td><code>b</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的标量，指定区间的上界</td>
</tr>
<tr>
<td><code>shape</code></td>
<td><code>Shape</code></td>
<td>类型为 T 的输出的形状</td>
</tr>
</tbody>
</table></div>
<h2>Select</h2>
<p>另请参阅<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Select</code></a>.</p>
<p>基于 predicate 数组的值，从两个输入数组构造输出数组。</p>
<p><b> `Select(pred, on_true, on_false)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>pred</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 PRED 的数组</td>
</tr>
<tr>
<td><code>on_true</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
<tr>
<td><code>on_false</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组</td>
</tr>
</tbody>
</table></div>
<p>数组 <code>on_true</code> 和 <code>on_false</code> 的形状必须相同。这也是输出数组的形状。数组 <code>pred</code> 必须与 <code>on_true</code>、 <code>on_false</code>具有相同的维度，且值为 <code>PRED</code> 类型。</p>
<p>对于 <code>pred</code> 的每个元素 <code>P</code>，当 <code>P</code> 值为 <code>true</code> 时，相应的输出值从 <code>on_true</code> 中获取，否则从 <code>on_false</code> 中获取。由于 <a href="broadcasting.md">broadcasting</a> 限制，<code>pred</code> 可以是类型为 <code>PRED</code> 的标量。此时，当 <code>pred</code> 值为 <code>true</code> 时，输出数组为 <code>on_true</code>，否则为 <code>on_false</code>。</p>
<p>非标量 <code>pred</code> 的示例如下：</p>
<pre><code>let pred: PRED[4] = {true, false, false, true};
let v1: s32[4] = {1, 2, 3, 4};
let v2: s32[4] = {100, 200, 300, 400};
==&gt;
Select(pred, v1, v2) = s32[4]{1, 200, 300, 4};
</code></pre>
<p>标量 <code>pred</code> 的示例如下：</p>
<pre><code>let pred: PRED = true;
let v1: s32[4] = {1, 2, 3, 4};
let v2: s32[4] = {100, 200, 300, 400};
==&gt;
Select(pred, v1, v2) = s32[4]{1, 2, 3, 4};
</code></pre>
<p>支持元组之间的 Selections 操作。因此元组认为是标量类型。如果 <code>on_true</code> 和  <code>on_false</code> 为元组（必须形状相同），则 <code>pred</code> 必须是类型为 <code>PRED</code> 的标量。</p>
<h2>SelectAndScatter</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::SelectAndScatter</code></a>。</p>
<p>这个操作可视为一个复合操作，它先在 <code>operand</code> 数组上计算 <code>ReduceWindow</code>，以便从每个窗口中选择一个数，然后将 <code>source</code> 数组散布到选定元素的指标位置上，从而构造出一个与 <code>operand</code> 数组形状一样的输出数组。二元函数 <code>select</code> 用于从每个窗口中选出一个元素，当调用此函数时，第一个参数的指标矢量的字典序小于第二个参数的指标矢量。如果第一个参数被选中，则 <code>select</code> 返回 <code>true</code>，如果第二个参数被选中，则返回 <code>false</code>。而且该函数必须满足传递性，即如果 <code>select(a, b)</code> 和 <code>select(b, c)</code> 都为 <code>true</code>，则 <code>select(a, c)</code> 也为 <code>true</code>。这样，被选中的元素不依赖于指定窗口中元素访问的顺序。</p>
<p><code>scatter</code> 函数作用在输出数组的每个选中的指标上。它有两个标量参数：</p>
<ol>
<li>输出数组中选中指标处的值</li>
<li><code>source</code> 中被放置到选中指标处的值</li>
</ol>
<p>它根据这两个参数返回一个标量值，用于更新输出数组中选中指标处的值。最开始的时候，输出数组所有指标处的值都被设为 <code>init_value</code>。</p>
<p>输出数组与 <code>operand</code> 数组的形状相同，而 <code>source</code> 数组必须与 <code>operand</code> 上应用 <code>ReduceWindow</code> 之后的形状相同。 <code>SelectAndScatter</code> 可用于神经网络池化层中梯度值的反向传播。</p>
<p><b>`SelectAndScatter(operand, select, window_dimensions, window_strides,
padding, source, init_value, scatter)`</b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组，窗口在它上面滑动</td>
</tr>
<tr>
<td><code>select</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T, T -&gt; PRED</code> 的二元计算，它被应用到每个窗口中的所有元素上；如果选中第一个元素返回 <code>true</code>，如果选中第二个元素返回 <code>false</code></td>
</tr>
<tr>
<td><code>window_dimensions</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>表示窗口维度值的整数数组</td>
</tr>
<tr>
<td><code>window_strides</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>表示窗口步长值的整数数组</td>
</tr>
<tr>
<td><code>padding</code></td>
<td><code>Padding</code></td>
<td>窗口边缘填充类型（Padding\:\:kSame 或 Padding\:\:kValid）</td>
</tr>
<tr>
<td><code>source</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的数组，它的值用于散布</td>
</tr>
<tr>
<td><code>init_value</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的标量值，用于输出数组的初值</td>
</tr>
<tr>
<td><code>scatter</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T, T -&gt; T</code> 的二元计算，应用于 source 的每个元素和它的目标元素</td>
</tr>
</tbody>
</table></div>
<p>下图为 <code>SelectAndScatter</code> 的示例，其中 <code>select</code> 函数计算它的参数中的最大值。注意，当窗口重叠时，如图 (2) 所示，<code>operand</code> 的一个指标可能会被不同窗口多次选中。在此图中，值为 9 的元素被顶部的两个窗口（蓝色和红色）选中，从而二元加法函数 <code>scatter</code> 产生值为 8 的输出值（2+6）。</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%"
    src="https://www.tensorflow.org/images/ops_scatter_to_selected_window_element.png">
</div><p><code>scatter</code> 函数的执行顺序是任意的，因而可能会出现不确定的结果。所以，<code>scatter</code> 函数不应该对计算的结合性过于敏感。更多细节，参见 <a href="#reduce"><code>Reduce</code></a> 一节中关于结合性的讨论。</p>
<h2>Send</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Send</code></a>。</p>
<p><b> `Send(operand, channel_handle)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>待发送的数据（类型为 T 的数组）</td>
</tr>
<tr>
<td><code>channel_handle</code></td>
<td><code>ChannelHandle</code></td>
<td>发送/接收 对的唯一标识符</td>
</tr>
</tbody>
</table></div>
<p>将给定的 operand 数据发送到另一台计算机上共享相同通道句柄的 <code>Recv</code> 中。不返回任何数据。</p>
<p>与 <code>Recv</code> 操纵类似，<code>Send</code> 操作的客户端 API 为同步通信，并在内部分解为 2 个 HLO 指令（<code>Send</code> 和 <code>SendDone</code>）以使用异步数据传输。另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/hlo_instruction.h"><code>HloInstruction::CreateSend</code> 和 <code>HloInstruction::CreateSendDone</code></a>。</p>
<p><b>`Send(HloInstruction operand, int64 channel_id)`</b></p>
<p>发起 operand 的异步传输过程，将数据传输到具有相同通道 id 的 <code>Recv</code> 指令分配的资源中。返回一个上下文，随后使用 <code>SendDone</code> 指令等待数据传输完成。上下文是 {operand (shape), request identifier<br>
(U32)} 的二元组，且只能用于 <code>SendDone</code> 指令。</p>
<p><b> `SendDone(HloInstruction context)` </b></p>
<p>根据 <code>Send</code> 指令创建的上下文，等待数据传输完成。指令不返回任何数据。</p>
<p><b> Scheduling of channel instructions </b></p>
<p>每个通道的 4 个指令 (<code>Recv</code>, <code>RecvDone</code>, <code>Send</code>, <code>SendDone</code>) 的执行顺序如下。</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:70%" src="../../images/send_recv_order.png">
</div><ul>
<li><code>Recv</code> happens before <code>Send</code></li>
<li><code>Send</code> happens before <code>RecvDone</code></li>
<li><code>Recv</code> happens before <code>RecvDone</code></li>
<li><code>Send</code> happens before <code>SendDone</code></li>
</ul>
<p>当后端编译器为通过通道指令进行通信的每一个计算生成一个线性调度时，在计算过程中不能有循环。例如，下面的调度会产生死循环。</p>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="../../images/send_recv_schedule.png">
</div><h2>Slice</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Slice</code></a>。</p>
<p><code>Slice</code> 用于从输入数组中提取出一个子数组。子数组与输入数组的秩相同，它的值在输入数组的包围盒中，此包围盒的维度和指标作为 slice 操作的参数而给出。</p>
<p><b> `Slice(operand, start_indices, limit_indices)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>类型为 T 的 N 维数组</td>
</tr>
<tr>
<td><code>start_indices</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>N 个整数的数组，包含每个维度的切片的起始指标。值必须大于等于零</td>
</tr>
<tr>
<td><code>limit_indices</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>N 个整数的数组，包含每个维度的切片的结束指标（不包含）。每个维度的结束指标必须严格大于其起始指标，且小于等于维度大小</td>
</tr>
</tbody>
</table></div>
<p>1-维示例：</p>
<pre><code>let a = {0.0, 1.0, 2.0, 3.0, 4.0}
Slice(a, {2}, {4}) produces:
  {2.0, 3.0}
</code></pre>
<p>2-维示例：</p>
<pre><code>let b =
 { {0.0,  1.0,  2.0},
   {3.0,  4.0,  5.0},
   {6.0,  7.0,  8.0},
   {9.0, 10.0, 11.0} }

Slice(b, {2, 1}, {4, 3}) produces:
  { { 7.0,  8.0},
    {10.0, 11.0} }
</code></pre>
<h2>Sort</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Sort</code></a>。</p>
<p><code>Sort</code> 用于对输入数组中的元素进行排序。</p>
<p><b>`Sort(operand)`</b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>待排序数组</td>
</tr>
</tbody>
</table></div>
<h2>Transpose</h2>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/reshape"><code>tf.reshape</code></a></p>
<p><b>`Transpose(operand)`</b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>operand</code></td>
<td><code>ComputationDataHandle</code></td>
<td>待转置的数组</td>
</tr>
<tr>
<td><code>permutation</code></td>
<td><code>ArraySlice&lt;int64&gt;</code></td>
<td>指定维度重排列的方式</td>
</tr>
</tbody>
</table></div>
<p>Transpose 将 operand 数组的维度重排列，所以<br>
<code>∀ i . 0 ≤ i &lt; rank ⇒ input_dimensions[permutation[i]] = output_dimensions[i]</code>。</p>
<p>这等价于 Reshape(operand, permutation, Permute(permutation, operand.shape.dimensions))。</p>
<h2>Tuple</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::Tuple</code></a>。</p>
<p>一个元组（tuple）包含一些数据句柄，它们各自都有自己的形状。</p>
<p>概念上看，它类似于 C++ 中的 <code>std::tuple</code>：</p>
<pre><code>let v: f32[10] = f32[10]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
let s: s32 = 5;
let t: (f32[10], s32) = tuple(v, s);
</code></pre>
<p>元组可通过 <a href="#gettupleelement"><code>GetTupleElement</code></a> 操作来解析（访问）。</p>
<h2>While</h2>
<p>另请参阅 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/client/computation_builder.h"><code>ComputationBuilder::While</code></a>。</p>
<p><b> `While(condition, body, init)` </b></p>
<div class="table-wrapper"><table>
<thead><tr>
<th>参数</th>
<th>类型</th>
<th>语义</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>condition</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T -&gt; PRED</code> 的计算，它定义了循环终止的条件</td>
</tr>
<tr>
<td><code>body</code></td>
<td><code>Computation</code></td>
<td>类型为 <code>T -&gt; T</code> 的计算，它定义了循环体</td>
</tr>
<tr>
<td><code>init</code></td>
<td><code>T</code></td>
<td><code>condition</code> 和 <code>body</code> 的参数的初始值</td>
</tr>
</tbody>
</table></div>
<p><code>While</code> 顺序执行循环体 <code>body</code> ，直到 <code>condition</code> 失败。这类似于很多语言中的 while 循环，不过，它有如下的区别和限制：</p>
<ul>
<li>一个 <code>While</code> 结点有一个类型为 <code>T</code> 的返回值，它是最后一次执行 <code>body</code> 的结果。</li>
<li>类型为 <code>T</code> 的形状是由统计确定的，在整个迭代过程中，它都是保持不变的。</li>
<li><code>While</code> 结点之间不允许嵌套。这个限制可能会在未来某些目标平台上取消。</li>
</ul>
<p>该计算的类型为 T 的那些参数使用 <code>init</code> 作为迭代的第一次计算的初值，并在接下来的迭代中由 <code>body</code> 来更新。</p>
<p><code>While</code> 结点的一个主要使用安例是实现神经网络中的训练的重复执行。下面是一个简化版的伪代码，和一个表示计算过程的图。实际代码可以在 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/tests/while_test.cc"><code>while_test.cc</code></a> 中找到。此例中的 <code>T</code> 类型为一个 <code>Tuple</code>，它包含一个 <code>int32</code> 值，表示迭代次数，还有一个 <code>vector[10]</code>，用于累加结果。它有 1000 次迭代，每一次都会将一个常数矢量累加到 result(1) 上。</p>
<pre><code>// Pseudocode for the computation.
init = {0, zero_vector[10]} // Tuple of int32 and float[10].
result = init;
while (result(0) &lt; 1000) {
  iteration = result(0) + 1;
  new_vector = result(1) + constant_vector[10];
  result = {iteration, new_vector};
}
</code></pre>
<div style="width:95%; margin:auto; margin-bottom:10px; margin-top:20px;">
  <img style="width:100%" src="https://www.tensorflow.org/images/ops_while.png">
</div>
        </main>
    </div>
</div>
<!-- Content end-->

<!-- Footer start -->
<footer class="footer">
    <div class="container">
        <div>如果您发现本页面存在错误或可以改进，请<a href="https://github.com/xitu/tensorflow-docs/blob/zh-hans/performance/xla/operation_semantics.md" target="_blank">点击此处</a>帮助我们改进。本页贡献者：<span id="contributors"></span></div>
        <hr/>
        <div class="text-center official-links">
            <a href="https://www.tensorflow.org"><img
                    src="https://www.tensorflow.org/_static/b1fb9a8564/images/tensorflow/lockup.png" height="20"/></a>
            <a href="https://github.com/xitu/tensorflow-docs"><img
                    src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Logo.png" height="20"></a>
            <a href="https://juejin.im"><img src="//xitu.github.io/tensorflow-docs-web/assets/imgs/logo_app_white.png" height="20"/></a>
        </div>
    </div>
</footer>
<script>
    var contributors = [{'徐键': ''}, {'leviding': 'https://avatars3.githubusercontent.com/u/26959437?v=4'}]
</script>
<!-- Footer end -->
</body>
<script src="//cdn.bootcss.com/jquery/3.3.1/jquery.slim.min.js" type="text/javascript"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js" type="text/javascript"></script>
<script src="//xitu.github.io/tensorflow-docs-web/assets/js/main.js" type="text/javascript"></script>
</html>