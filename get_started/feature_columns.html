<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>Feature Columns</title>
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/main.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
</head>
<body>
<!-- Header start -->
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="#">TensorFlow</a>
    <button class="navbar-toggler" type="button" aria-expanded="false" aria-label="Menu"
            onclick="$('.collapse').toggle()">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse">
        <ul class="navbar-nav mr-auto">
        </ul>
        <!-- TODO: Search function-->
        <!--<form class="form-inline my-2 my-lg-0">-->
            <!--<input class="form-control mr-sm-2" type="search" placeholder="Search" aria-label="Search">-->
            <!--<button class="btn btn-outline-success my-2 my-sm-0" type="submit">Search</button>-->
        <!--</form>-->
    </div>
</nav>
<script>
    var head = [{'link': '//xitu.github.io/tensorflow-docs-web/extend/index.html', 'name': '扩展', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/install/index.html', 'name': '安装 TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/deploy/index.html', 'name': '部署', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/about/index.html', 'name': 'About TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/index.html', 'name': '开始', 'selected': 1}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/index.html', 'name': '概述', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/tutorials/index.html', 'name': '教程', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/index.html', 'name': '性能', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/community/index.html', 'name': 'Community', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/programmers_guide/index.html', 'name': '开发者指南', 'selected': 0}]
</script>
<!-- Header end -->

<!-- Content start-->
<div class="container-fluid">
    <div class="row">
        <nav class="col-md-2 d-none d-md-block bg-light sidebar">
    <div class="sidebar-sticky" id="left-nav">

    </div>
</nav>
<script>
    var nav = [{'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/get_started/index.html', 'title': '开始'}, {'type': 'parent', 'title': ' 入门', 'sub_class': [{'link': '//xitu.github.io/tensorflow-docs-web/get_started/get_started_for_beginners.html', 'title': '机器学习新手入门'}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/premade_estimators.html', 'title': 'TensorFlow 入门教程'}]}, {'type': 'parent', 'title': ' 细节', 'sub_class': [{'link': '//xitu.github.io/tensorflow-docs-web/get_started/checkpoints.html', 'title': '检查点'}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/feature_columns.html', 'title': 'Feature Columns'}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/datasets_quickstart.html', 'title': '数据集：快速了解'}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/custom_estimators.html', 'title': 'Creating Custom Estimators'}]}]
</script>
        <main role="main" class="col-md-9 ml-sm-auto col-lg-10 pt-3 px-4">
            <h1>Feature Columns</h1>
<p>This document details feature columns. Think of <strong>feature columns</strong> as the<br>
intermediaries between raw data and Estimators. Feature columns are very rich,<br>
enabling you to transform a diverse range of raw data into formats that<br>
Estimators can use, allowing easy experimentation.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column"><code>tf.feature_column.numeric_column</code></a></p>
<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/feature_cloud.jpg">
</div>
<div style="text-align: center">
Some real-world features (such as, longitude) are numerical, but many are not.
</div><h2>Input to a Deep Neural Network</h2>
<p>What kind of data can a deep neural network operate on? The answer<br>
is, of course, numbers (for example, <code>tf.float32</code>). After all, every neuron in<br>
a neural network performs multiplication and addition operations on weights and<br>
input data. Real-life input data, however, often contains non-numerical<br>
(categorical) data. For example, consider a <code>product_class</code> feature that can<br>
contain the following three non-numerical values:</p>
<ul>
<li><code>kitchenware</code></li>
<li><code>electronics</code></li>
<li><code>sports</code></li>
</ul>
<p>ML models generally represent categorical values as simple vectors in which a<br>
1 represents the presence of a value and a 0 represents the absence of a value.<br>
For example, when <code>product_class</code> is set to <code>sports</code>, an ML model would usually<br>
represent <code>product_class</code> as  <code>[0, 0, 1]</code>, meaning:</p>
<ul>
<li><code>0</code>: <code>kitchenware</code> is absent</li>
<li><code>0</code>: <code>electronics</code> is absent</li>
<li><code>1</code>: <code>sports</code> is present</li>
</ul>
<p>So, although raw data can be numerical or categorical, an ML model represents<br>
all features as numbers.</p>
<h2>Feature Columns</h2>
<p>As the following figure suggests, you specify the input to a model through the<br>
<code>feature_columns</code> argument of an Estimator (<code>DNNClassifier</code> for Iris).<br>
Feature Columns bridge input data (as returned by <code>input_fn</code>) with your model.</p>
<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/inputs_to_model_bridge.jpg">
</div>
<div style="text-align: center">
Feature columns bridge raw data with the data your model needs.
</div><p><a href="https://www.tensorflow.org/api_docs/python/tf/feature_column"><code>tf.feature_column</code></a></p>
<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/some_constructors.jpg">
</div>
<div style="text-align: center">
Feature column methods fall into two main categories and one hybrid category.
</div><p>Let's look at these functions in more detail.</p>
<h3>Numeric column</h3>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column"><code>tf.feature_column.numeric_column</code></a></p>
<ul>
<li><code>SepalLength</code></li>
<li><code>SepalWidth</code></li>
<li><code>PetalLength</code></li>
<li><code>PetalWidth</code></li>
</ul>
<p>Although <code>tf.numeric_column</code> provides optional arguments, calling<br>
<code>tf.numeric_column</code> without any arguments, as follows, is a fine way to specify<br>
a numerical value with the default data type (<code>tf.float32</code>) as input to your<br>
model:</p>
<pre><code class="lang-python"># Defaults to a tf.float32 scalar.
numeric_feature_column = tf.feature_column.numeric_column(key=&quot;SepalLength&quot;)
</code></pre>
<p>To specify a non-default numerical data type, use the <code>dtype</code> argument. For<br>
example:</p>
<pre><code class="lang-python"># Represent a tf.float64 scalar.
numeric_feature_column = tf.feature_column.numeric_column(key=&quot;SepalLength&quot;,
                                                          dtype=tf.float64)
</code></pre>
<p>By default, a numeric column creates a single value (scalar). Use the shape<br>
argument to specify another shape. For example:</p>
<!--TODO(markdaoust) link to full example-->
```python
# Represent a 10-element vector in which each cell contains a tf.float32.
vector_feature_column = tf.feature_column.numeric_column(key="Bowling",
                                                         shape=10)

# Represent a 10x5 matrix in which each cell contains a tf.float32.
matrix_feature_column = tf.feature_column.numeric_column(key="MyMatrix",
                                                         shape=[10,5])
```
### Bucketized column

Often, you don't want to feed a number directly into the model, but instead
split its value into different categories based on numerical ranges.  To do so,
create a @{tf.feature_column.bucketized_column$bucketized column}. For
example, consider raw data that represents the year a house was built. Instead
of representing that year as a scalar numeric column, we could split the year
into the following four buckets:

<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/bucketized_column.jpg">
</div>
<div style="text-align: center">
Dividing year data into four buckets.
</div>

The model will represent the buckets as follows:

|Date Range |Represented as... |
|:----------|:-----------------|
|< 1960               | [1, 0, 0, 0] |
|>= 1960 but < 1980   | [0, 1, 0, 0] |
|>= 1980 but < 2000   | [0, 0, 1, 0] |
|> 2000               | [0, 0, 0, 1] |

Why would you want to split a number—a perfectly valid input to your
model—into a categorical value? Well, notice that the categorization splits a
single input number into a four-element vector. Therefore, the model now can
learn _four individual weights_ rather than just one; four weights creates a
richer model than one weight. More importantly, bucketizing enables the model
to clearly distinguish between different year categories since only one of the
elements is set (1) and the other three elements are cleared (0). When we just
use a single number (a year) as input, the model can only learn a linear
relationship. So, bucketing provides the model with additional flexibility that
the model can use to learn.

The following code demonstrates how to create a bucketized feature:

<!--TODO(markdaoust) link to full example - housing price grid?-->
```python
# First, convert the raw input to a numeric column.
numeric_feature_column = tf.feature_column.numeric_column("Year")

# Then, bucketize the numeric column on the years 1960, 1980, and 2000.
bucketized_feature_column = tf.feature_column.bucketized_column(
    source_column = numeric_feature_column,
    boundaries = [1960, 1980, 2000])
```
Note that specifying a _three_-element boundaries vector creates a
_four_-element bucketized vector.


### Categorical identity column

**Categorical identity columns** can be seen as a special case of bucketized
columns. In traditional bucketized columns, each bucket represents a range of
values (for example, from 1960 to 1979). In a categorical identity column, each
bucket represents a single, unique integer. For example, let's say you want to
represent the integer range `[0, 4)`.  That is, you want to represent the
integers 0, 1, 2, or 3. In this case, the categorical identity mapping looks
like this:

<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/categorical_column_with_identity.jpg">
</div>
<div style="text-align: center">
A categorical identity column mapping. Note that this is a one-hot
encoding, not a binary numerical encoding.
</div>

As with bucketized columns, a model can learn a separate weight for each class
in a categorical identity column. For example, instead of using a string to
represent the `product_class`, let's represent each class with a unique integer
value. That is:

* `0="kitchenware"`
* `1="electronics"`
* `2="sport"`

Call @{tf.feature_column.categorical_column_with_identity} to implement a
categorical identity column. For example:

``` python
# Create categorical output for an integer feature named "my_feature_b",
# The values of my_feature_b must be >= 0 and < num_buckets
identity_feature_column = tf.feature_column.categorical_column_with_identity(
    key='my_feature_b',
    num_buckets=4) # Values [0, 4)

# In order for the preceding call to work, the input_fn() must return
# a dictionary containing 'my_feature_b' as a key. Furthermore, the values
# assigned to 'my_feature_b' must belong to the set [0, 4).
def input_fn():
    ...
    return ({ 'my_feature_a':[7, 9, 5, 2], 'my_feature_b':[3, 1, 2, 2] },
            [Label_values])
```

### Categorical vocabulary column

We cannot input strings directly to a model. Instead, we must first map strings
to numeric or categorical values. Categorical vocabulary columns provide a good
way to represent strings as a one-hot vector. For example:

<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/categorical_column_with_vocabulary.jpg">
</div>
<div style="text-align: center">
Mapping string values to vocabulary columns.
</div>

As you can see, categorical vocabulary columns are kind of an enum version of
categorical identity columns. TensorFlow provides two different functions to
create categorical vocabulary columns:

* @{tf.feature_column.categorical_column_with_vocabulary_list}
* @{tf.feature_column.categorical_column_with_vocabulary_file}

`categorical_column_with_vocabulary_list` maps each string to an integer based
on an explicit vocabulary list. For example:

```python
# Given input "feature_name_from_input_fn" which is a string,
# create a categorical feature by mapping the input to one of
# the elements in the vocabulary list.
vocabulary_feature_column =
    tf.feature_column.categorical_column_with_vocabulary_list(
        key="a feature returned by input_fn()",
        vocabulary_list=["kitchenware", "electronics", "sports"])
```

The preceding function is pretty straightforward, but it has a significant
drawback. Namely, there's way too much typing when the vocabulary list is long.
For these cases, call
`tf.feature_column.categorical_column_with_vocabulary_file` instead, which lets
you place the vocabulary words in a separate file. For example:

```python

# Given input "feature_name_from_input_fn" which is a string,
# create a categorical feature to our model by mapping the input to one of
# the elements in the vocabulary file
vocabulary_feature_column =
    tf.feature_column.categorical_column_with_vocabulary_file(
        key="a feature returned by input_fn()",
        vocabulary_file="product_class.txt",
        vocabulary_size=3)
```

`product_class.txt` should contain one line for each vocabulary element. In our
case:

```None
kitchenware
electronics
sports
```

### Hashed Column

So far, we've worked with a naively small number of categories. For example,
our product_class example has only 3 categories. Often though, the number of
categories can be so big that it's not possible to have individual categories
for each vocabulary word or integer because that would consume too much memory.
For these cases, we can instead turn the question around and ask, "How many
categories am I willing to have for my input?"  In fact, the
@{tf.feature_column.categorical_column_with_hash_bucket} function enables you
to specify the number of categories. For this type of feature column the model
calculates a hash value of the input, then puts it into one of
the `hash_bucket_size` categories using the modulo operator, as in the following
pseudocode:

```python
# pseudocode
feature_id = hash(raw_feature) % hash_buckets_size
```

The code to create the `feature_column` might look something like this:

``` python
hashed_feature_column =
    tf.feature_column.categorical_column_with_hash_bucket(
        key = "some_feature",
        hash_buckets_size = 100) # The number of categories
```
At this point, you might rightfully think: "This is crazy!" After all, we are
forcing the different input values to a smaller set of categories. This means
that two probably unrelated inputs will be mapped to the same
category, and consequently mean the same thing to the neural network. The
following figure illustrates this dilemma, showing that kitchenware and sports
both get assigned to category (hash bucket) 12:

<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/hashed_column.jpg">
</div>
<div style="text-align: center">
Representing data with hash buckets.
</div>

As with many counterintuitive phenomena in machine learning, it turns out that
hashing often works well in practice. That's because hash categories provide
the model with some separation. The model can use additional features to further
separate kitchenware from sports.

### Crossed column

Combining features into a single feature, better known as
[feature crosses](https://developers.google.com/machine-learning/glossary/#feature_cross),
enables the model to learn separate weights for each combination of
features.

More concretely, suppose we want our model to calculate real estate prices in
Atlanta, GA. Real-estate prices within this city vary greatly depending on
location. Representing latitude and longitude as separate features isn't very
useful in identifying real-estate location dependencies; however, crossing
latitude and longitude into a single feature can pinpoint locations. Suppose we
represent Atlanta as a grid of 100x100 rectangular sections, identifying each
of the 10,000 sections by a feature cross of latitude and longitude. This
feature cross enables the model to train on pricing conditions related to each
individual section, which is a much stronger signal than latitude and longitude
alone.

The following figure shows our plan, with the latitude & longitude values for
the corners of the city in red text:

<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/Atlanta.jpg">
</div>
<div style="text-align: center">
Map of Atlanta. Imagine this map divided into 10,000 sections of
equal size.
</div>

For the solution, we used a combination of the `bucketized_column` we looked at
earlier, with the @{tf.feature_column.crossed_column} function.

<!--TODO(markdaoust) link to full example-->

<pre><code class="lang-python">def make_dataset(latitude, longitude, labels):
    assert latitude.shape == longitude.shape == labels.shape

    features = {&#39;latitude&#39;: latitude.flatten(),
                &#39;longitude&#39;: longitude.flatten()}
    labels=labels.flatten()

    return tf.data.Dataset.from_tensor_slices((features, labels))


# Bucketize the latitude and longitude usig the `edges`
latitude_bucket_fc = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column(&#39;latitude&#39;),
    list(atlanta.latitude.edges))

longitude_bucket_fc = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column(&#39;longitude&#39;),
    list(atlanta.longitude.edges))

# Cross the bucketized columns, using 5000 hash bins.
crossed_lat_lon_fc = tf.feature_column.crossed_column(
    [latitude_bucket_fc, longitude_bucket_fc], 5000)

fc = [
    latitude_bucket_fc,
    longitude_bucket_fc,
    crossed_lat_lon_fc]

# Build and train the Estimator.
est = tf.estimator.LinearRegressor(fc, ...)
</code></pre>
<p>You may create a feature cross from either of the following:</p>
<ul>
<li>Feature names; that is, names from the <code>dict</code> returned from <code>input_fn</code>.</li>
<li>Any categorical column, except <code>categorical_column_with_hash_bucket</code><br>
(since <code>crossed_column</code> hashes the input).</li>
</ul>
<p>When the feature columns <code>latitude_bucket_fc</code> and <code>longitude_bucket_fc</code> are<br>
crossed, TensorFlow will create <code>(latitude_fc, longitude_fc)</code> pairs for each<br>
example. This would produce a full grid of possibilities as follows:</p>
<pre><code class="lang-None"> (0,0),  (0,1)...  (0,99)
 (1,0),  (1,1)...  (1,99)
   ...     ...       ...
(99,0), (99,1)...(99, 99)
</code></pre>
<p>Except that a full grid would only be tractable for inputs with limited<br>
vocabularies. Instead of building this, potentially huge, table of inputs,<br>
the <code>crossed_column</code> only builds the number requested by the <code>hash_bucket_size</code><br>
argument. The feature column assigns an example to a index by running a hash<br>
function on the tuple of inputs, followed by a modulo operation with<br>
<code>hash_bucket_size</code>.</p>
<p>As discussed earlier, performing the<br>
hash and modulo function limits the number of categories, but can cause category<br>
collisions; that is, multiple (latitude, longitude) feature crosses will end<br>
up in the same hash bucket. In practice though, performing feature crosses<br>
still adds significant value to the learning capability of your models.</p>
<p>Somewhat counterintuitively, when creating feature crosses, you typically still<br>
should include the original (uncrossed) features in your model (as in the<br>
preceding code snippet). The independent latitude and longitude features help the<br>
model distinguish between examples where a hash collision has occurred in the<br>
crossed feature.</p>
<h2>Indicator and embedding columns</h2>
<p>Indicator columns and embedding columns never work on features directly, but<br>
instead take categorical columns as input.</p>
<p>When using an indicator column, we're telling TensorFlow to do exactly what<br>
we've seen in our categorical product_class example. That is, an<br>
<strong>indicator column</strong> treats each category as an element in a one-hot vector,<br>
where the matching category has value 1 and the rest have 0s:</p>
<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/categorical_column_with_identity.jpg">
</div>
<div style="text-align: center">
Representing data in indicator columns.
</div><p><a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column"><code>tf.feature_column.indicator_column</code></a></p>
<pre><code class="lang-python">categorical_column = ... # Create any type of categorical column.

# Represent the categorical column as an indicator column.
indicator_column = tf.feature_column.indicator_column(categorical_column)
</code></pre>
<p>Now, suppose instead of having just three possible classes, we have a million.<br>
Or maybe a billion. For a number of reasons, as the number of categories grow<br>
large, it becomes infeasible to train a neural network using indicator columns.</p>
<p>We can use an embedding column to overcome this limitation. Instead of<br>
representing the data as a one-hot vector of many dimensions, an<br>
<strong>embedding column</strong> represents that data as a lower-dimensional, ordinary<br>
vector in which each cell can contain any number, not just 0 or 1. By<br>
permitting a richer palette of numbers for every cell, an embedding column<br>
contains far fewer cells than an indicator column.</p>
<p>Let's look at an example comparing indicator and embedding columns. Suppose our<br>
input examples consist of different words from a limited palette of only 81<br>
words. Further suppose that the data set provides the following input<br>
words in 4 separate examples:</p>
<ul>
<li><code>"dog"</code></li>
<li><code>"spoon"</code></li>
<li><code>"scissors"</code></li>
<li><code>"guitar"</code></li>
</ul>
<p>In that case, the following figure illustrates the processing path for<br>
embedding columns or indicator columns.</p>
<div style="width:80%; margin:auto; margin-bottom:10px; margin-top:20px;">
<img style="width:100%" src="../images/feature_columns/embedding_vs_indicator.jpg">
</div>
<div style="text-align: center">
An embedding column stores categorical data in a lower-dimensional
vector than an indicator column. (We just placed random numbers into the
embedding vectors; training determines the actual numbers.)
</div><p>When an example is processed, one of the <code>categorical_column_with...</code> functions<br>
maps the example string to a numerical categorical value. For example, a<br>
function maps "spoon" to <code>[32]</code>. (The 32 comes from our imagination—the actual<br>
values depend on the mapping function.) You may then represent these numerical<br>
categorical values in either of the following two ways:</p>
<ul>
<li><p>As an indicator column. A function converts each numeric categorical value<br>
into an 81-element vector (because our palette consists of 81 words), placing<br>
a 1 in the index of the categorical value (0, 32, 79, 80) and a 0 in all the<br>
other positions.</p>
</li>
<li><p>As an embedding column. A function uses the numerical categorical values<br>
<code>(0, 32, 79, 80)</code> as indices to a lookup table. Each slot in that lookup table<br>
contains a 3-element vector.</p>
</li>
</ul>
<p>How do the values in the embeddings vectors magically get assigned? Actually,<br>
the assignments happen during training. That is, the model learns the best way<br>
to map your input numeric categorical values to the embeddings vector value in<br>
order to solve your problem. Embedding columns increase your model's<br>
capabilities, since an embeddings vector learns new relationships between<br>
categories from the training data.</p>
<p>Why is the embedding vector size 3 in our example? Well, the following "formula"<br>
provides a general rule of thumb about the number of embedding dimensions:</p>
<pre><code class="lang-python">embedding_dimensions =  number_of_categories**0.25
</code></pre>
<p>That is, the embedding vector dimension should be the 4th root of the number of<br>
categories. Since our vocabulary size in this example is 81, the recommended<br>
number of dimensions is 3:</p>
<pre><code class="lang-python">3 =  81**0.25
</code></pre>
<p>Note that this is just a general guideline; you can set the number of embedding<br>
dimensions as you please.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column"><code>tf.feature_column.embedding_column</code></a></p>
<pre><code class="lang-python">categorical_column = ... # Create any categorical column

# Represent the categorical column as an embedding column.
# This means creating a one-hot vector with one element for each category.
embedding_column = tf.feature_column.embedding_column(
    categorical_column=categorical_column,
    dimension=dimension_of_embedding_vector)
</code></pre>
<p><a href="//xitu.github.io/tensorflow-docs-web/programmers_guide/embedding.html">Embeddings</a></p>
<h2>Passing feature columns to Estimators</h2>
<p>As the following list indicates, not all Estimators permit all types of<br>
<code>feature_columns</code> argument(s):</p>
<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor"><code>tf.estimator.LinearRegressor</code></a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor"><code>tf.estimator.DNNRegressor</code></a></li>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedRegressor"><code>tf.estimator.DNNLinearCombinedRegressor</code></a><ul>
<li>The <code>linear_feature_columns</code> argument accepts any feature column type.</li>
<li>The <code>dnn_feature_columns</code> argument only accepts dense columns.</li>
</ul>
</li>
</ul>
<h2>Other Sources</h2>
<p>For more examples on feature columns, view the following:</p>
<ul>
<li><a href="//xitu.github.io/tensorflow-docs-web/programmers_guide/low_level_intro.html#feature_columns">底层 API 编程介绍</a></li>
<li><a href="//xitu.github.io/tensorflow-docs-web/tutorials/wide_and_deep.html">TensorFlow 宽深学习</a></li>
</ul>
<p>To learn more about embeddings, see the following:</p>
<ul>
<li><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Deep Learning, NLP, and representations</a><br>
(Chris Olah's blog)</li>
<li>The TensorFlow <a href="http://projector.tensorflow.org">Embedding Projector</a></li>
</ul>

        </main>
    </div>
</div>
<!-- Content end-->

<!-- Footer start -->
<footer class="footer">
    <div class="container">
        <div>如果您发现本页面存在错误或可以改进，请<a href="https://github.com/xitu/tensorflow-docs/blob/zh-hans/get_started/feature_columns.md" target="_blank">点击此处</a>帮助我们改进。本页贡献者：<span id="contributors"></span></div>
        <hr/>
        <div class="text-center official-links">
            <a href="https://www.tensorflow.org"><img
                    src="https://www.tensorflow.org/_static/b1fb9a8564/images/tensorflow/lockup.png" height="20"/></a>
            <a href="https://github.com/xitu/tensorflow-docs"><img
                    src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Logo.png" height="20"></a>
            <a href="https://juejin.im"><img src="//xitu.github.io/tensorflow-docs-web/assets/imgs/logo_app_white.png" height="20"/></a>
        </div>
    </div>
</footer>
<script>
    var contributors = [{'leviding': 'https://avatars3.githubusercontent.com/u/26959437?v=4'}]
</script>
<!-- Footer end -->
</body>
<script src="//cdn.bootcss.com/jquery/3.3.1/jquery.slim.min.js" type="text/javascript"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js" type="text/javascript"></script>
<script src="//xitu.github.io/tensorflow-docs-web/assets/js/main.js" type="text/javascript"></script>
</html>