<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>Eager Execution</title>
    <link href="//tensorflow.juejin.im/assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="//tensorflow.juejin.im/assets/css/main.css" rel="stylesheet">
    <link rel="stylesheet" href="//tensorflow.juejin.im/assets/css/highlight.js.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
</head>
<body>
<!-- Header start -->
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="//tensorflow.juejin.im">TensorFlow</a>
    <button class="navbar-toggler" type="button" aria-expanded="false" aria-label="Menu"
            onclick="$('.collapse').toggle()">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse">
        <ul class="navbar-nav mr-auto">
        </ul>
        <form class="form-inline my-2 my-lg-0 my-md-0">
            <input class="form-control mr-sm-2" id="search" type="search" placeholder="Search" aria-label="Search">
        </form>
    </div>
</nav>
<script>
    var head = [{'link': '//tensorflow.juejin.im/extend/index.html', 'name': '扩展', 'selected': 0}, {'link': '//tensorflow.juejin.im/install/index.html', 'name': '安装 TensorFlow', 'selected': 0}, {'link': '//tensorflow.juejin.im/deploy/index.html', 'name': '部署', 'selected': 0}, {'link': '//tensorflow.juejin.im/about/index.html', 'name': '关于 TensorFlow', 'selected': 0}, {'link': '//tensorflow.juejin.im/get_started/index.html', 'name': '开始', 'selected': 0}, {'link': '//tensorflow.juejin.im/mobile/index.html', 'name': '概述', 'selected': 0}, {'link': '//tensorflow.juejin.im/tutorials/index.html', 'name': '教程', 'selected': 0}, {'link': '//tensorflow.juejin.im/javascript/index.html', 'name': 'JavaScript', 'selected': 0}, {'link': '//tensorflow.juejin.im/performance/index.html', 'name': '性能', 'selected': 0}, {'link': '//tensorflow.juejin.im/community/index.html', 'name': '社区', 'selected': 0}, {'link': '//tensorflow.juejin.im/programmers_guide/index.html', 'name': '开发者指南', 'selected': 1}]
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-92630037-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-92630037-8');
</script>

<!-- Header end -->

<!-- Content start-->
<div class="container-fluid">
    <div class="row">
        <nav class="col-md-2 d-none d-md-block bg-light sidebar">
    <div class="sidebar-sticky" id="left-nav">

    </div>
</nav>
<script>
    var nav = [{'type': 'child', 'link': '//tensorflow.juejin.im/programmers_guide/index.html', 'title': '开发者指南'}, {'type': 'parent', 'title': ' High Level APIs', 'sub_class': [{'link': '//tensorflow.juejin.im/programmers_guide/eager.html', 'title': 'Eager Execution'}, {'link': '//tensorflow.juejin.im/programmers_guide/datasets.html', 'title': '数据导入'}, {'link': '//tensorflow.juejin.im/programmers_guide/estimators.html', 'title': '评估器'}]}, {'type': 'parent', 'title': ' Low Level APIs', 'sub_class': [{'link': '//tensorflow.juejin.im/programmers_guide/low_level_intro.html', 'title': '底层 API 编程介绍'}, {'link': '//tensorflow.juejin.im/programmers_guide/tensors.html', 'title': '张量'}, {'link': '//tensorflow.juejin.im/programmers_guide/variables.html', 'title': '变量'}, {'link': '//tensorflow.juejin.im/programmers_guide/graphs.html', 'title': '流图与会话'}, {'link': '//tensorflow.juejin.im/programmers_guide/saved_model.html', 'title': '保存和恢复'}]}, {'type': 'parent', 'title': ' Accelerators', 'sub_class': [{'link': '//tensorflow.juejin.im/programmers_guide/using_gpu.html', 'title': '使用（多个）GPU'}, {'link': '//tensorflow.juejin.im/programmers_guide/using_tpu.html', 'title': '使用 TPU'}]}, {'type': 'parent', 'title': ' ML Concepts', 'sub_class': [{'link': '//tensorflow.juejin.im/programmers_guide/embedding.html', 'title': 'Embeddings'}]}, {'type': 'parent', 'title': ' Debugging', 'sub_class': [{'link': '//tensorflow.juejin.im/programmers_guide/debugger.html', 'title': 'TensorFlow 调试器'}]}, {'type': 'parent', 'title': ' TensorBoard', 'sub_class': [{'link': '//tensorflow.juejin.im/programmers_guide/summaries_and_tensorboard.html', 'title': 'Tensorboard：可视化学习面板'}, {'link': '//tensorflow.juejin.im/programmers_guide/graph_viz.html', 'title': 'TensorBoard: 图形可视化'}, {'link': '//tensorflow.juejin.im/programmers_guide/tensorboard_histograms.html', 'title': 'TensorBoard 直方图面板'}]}, {'type': 'parent', 'title': ' Misc', 'sub_class': [{'link': '//tensorflow.juejin.im/programmers_guide/version_compat.html', 'title': 'TensorFlow 版本兼容性'}, {'link': '//tensorflow.juejin.im/programmers_guide/faq.html', 'title': '常见问题'}]}]
</script>
        <main role="main" class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 bd-content">
            <h1 id="toc-0">Eager Execution</h1>
<p>TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: operations return concrete values instead of constructing a computational graph to run later. This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well. To follow along with this guide, run the code samples below in an interactive <code>python</code> interpreter.</p>
<p>Eager execution is a flexible machine learning platform for research and experimentation, providing:</p>
<ul>
<li><em>An intuitive interface</em>—Structure your code naturally and use Python data structures. Quickly iterate on small models and small data.</li>
<li><em>Easier debugging</em>—Call ops directly to inspect running models and test changes. Use standard Python debugging tools for immediate error reporting.</li>
<li><em>Natural control flow</em>—Use Python control flow instead of graph control flow, simplifying the specification of dynamic models.</li>
</ul>
<p>Eager execution supports most TensorFlow operations and GPU acceleration. For a collection of examples running in eager execution, see: <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples">tensorflow/contrib/eager/python/examples</a>.</p>
<p>Note: Some models may experience increased overhead with eager execution enabled. Performance improvements are ongoing, but please <a href="https://github.com/tensorflow/tensorflow/issues">file a bug</a> if you find a problem and share your benchmarks.</p>
<h2 id="toc-1">Setup and basic usage</h2>
<p>Upgrade to the latest version of TensorFlow:</p>
<pre><code>$ pip install --upgrade tensorflow
</code></pre>
<p>To start eager execution, add <code>tf.enable_eager_execution()</code> to the beginning of the program or console session. Do not add this operation to other modules that the program calls.</p>
<pre><code class="lang-py">from __future__ import absolute_import, division, print_function

import tensorflow as tf

tf.enable_eager_execution()
</code></pre>
<p>Now you can run TensorFlow operations and the results will return immediately:</p>
<pre><code class="lang-py">tf.executing_eagerly()        # =&gt; True

x = [[2.]]
m = tf.matmul(x, x)
print(&quot;hello, {}&quot;.format(m))  # =&gt; &quot;hello, [[4.]]&quot;
</code></pre>
<p>Enabling eager execution changes how TensorFlow operations behave—now they immediately evaluate and return their values to Python. <code>tf.Tensor</code> objects reference concrete values instead of symbolic handles to nodes in a computational graph. Since there isn't a computational graph to build and run later in a session, it's easy to inspect results using <code>print()</code> or a debugger. Evaluating, printing, and checking tensor values does not break the flow for computing gradients.</p>
<p>Eager execution works nicely with <a href="http://www.numpy.org/">NumPy</a>. NumPy operations accept <code>tf.Tensor</code> arguments. TensorFlow <a href="https://www.tensorflow.org/api_guides/python/math_ops">math operations</a> convert Python objects and NumPy arrays to <code>tf.Tensor</code> objects. The <code>tf.Tensor.numpy</code> method returns the object's value as a NumPy <code>ndarray</code>.</p>
<pre><code class="lang-py">a = tf.constant([[1, 2],
                 [3, 4]])
print(a)
# =&gt; tf.Tensor([[1 2]
#               [3 4]], shape=(2, 2), dtype=int32)

# Broadcasting support
b = tf.add(a, 1)
print(b)
# =&gt; tf.Tensor([[2 3]
#               [4 5]], shape=(2, 2), dtype=int32)

# Operator overloading is supported
print(a * b)
# =&gt; tf.Tensor([[ 2  6]
#               [12 20]], shape=(2, 2), dtype=int32)

# Use NumPy values
import numpy as np

c = np.multiply(a, b)
print(c)
# =&gt; [[ 2  6]
#     [12 20]]

# Obtain numpy value from a tensor:
print(a.numpy())
# =&gt; [[1 2]
#     [3 4]]
</code></pre>
<p>The <code>tf.contrib.eager</code> module contains symbols available to both eager and graph execution environments and is useful for writing code to <a href="#work_with_graphs">work with graphs</a>:</p>
<pre><code class="lang-py">tfe = tf.contrib.eager
</code></pre>
<h2 id="toc-2">Dynamic control flow</h2>
<p>A major benefit of eager execution is that all the functionality of the host language is available while your model is executing. So, for example, it is easy to write <a href="https://en.wikipedia.org/wiki/Fizz_buzz">fizzbuzz</a>:</p>
<pre><code class="lang-py">def fizzbuzz(max_num):
  counter = tf.constant(0)
  for num in range(max_num):
    num = tf.constant(num)
    if num % 3 == 0 and num % 5 == 0:
      print(&#39;FizzBuzz&#39;)
    elif num % 3 == 0:
      print(&#39;Fizz&#39;)
    elif num % 5 == 0:
      print(&#39;Buzz&#39;)
    else:
      print(num)
    counter += 1
  return counter
</code></pre>
<p>This has conditionals that depend on tensor values and it prints these values at runtime.</p>
<h2 id="toc-3">Build a model</h2>
<p>Many machine learning models are represented by composing layers. When using TensorFlow with eager execution you can either write your own layers or use a layer provided in the <code>tf.keras.layers</code> package.</p>
<p>While you can use any Python object to represent a layer, TensorFlow has <code>tf.keras.layers.Layer</code> as a convenient base class. Inherit from it to implement your own layer:</p>
<pre><code class="lang-py">class MySimpleLayer(tf.keras.layers.Layer):
  def __init__(self, output_units):
    self.output_units = output_units

  def build(self, input):
    # The build method gets called the first time your layer is used.
    # Creating variables on build() allows you to make their shape depend on the input shape and hence remove the need for the user to specify full shapes. It is possible to create variables during __init__() if you already know their full shapes.
    self.kernel = self.add_variable(
      &quot;kernel&quot;, [input.shape[-1], self.output_units])

  def call(self, input):
    # Override call() instead of __call__ so we can perform some bookkeeping.
    return tf.matmul(input, self.kernel)
</code></pre>
<p>Use <code>tf.keras.layers.Dense</code> layer instead  of <code>MySimpleLayer</code> above as it has a superset of its functionality (it can also add a bias).</p>
<p>When composing layers into models you can use <code>tf.keras.Sequential</code> to represent models which are a linear stack of layers. It is easy to use for basic models:</p>
<pre><code class="lang-py">model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, input_shape=(784,)),  # must declare input shape
  tf.keras.layers.Dense(10)
])
</code></pre>
<p>Alternatively, organize models in classes by inheriting from <code>tf.keras.Model</code>. This is a container for layers that is a layer itself, allowing <code>tf.keras.Model</code> objects to contain other <code>tf.keras.Model</code> objects.</p>
<pre><code class="lang-py">class MNISTModel(tf.keras.Model):
  def __init__(self):
    super(MNISTModel, self).__init__()
    self.dense1 = tf.keras.layers.Dense(units=10)
    self.dense2 = tf.keras.layers.Dense(units=10)

  def call(self, input):
    &quot;&quot;&quot;Run the model.&quot;&quot;&quot;
    result = self.dense1(input)
    result = self.dense2(result)
    result = self.dense2(result)  # reuse variables from dense2 layer
    return result

model = MNISTModel()
</code></pre>
<p>It's not required to set an input shape for the <code>tf.keras.Model</code> class since the parameters are set the first time input is passed to the layer.</p>
<p><code>tf.keras.layers</code> classes create and contain their own model variables that are tied to the lifetime of their layer objects. To share layer variables, share their objects.</p>
<h2 id="toc-4">Eager training</h2>
<h3 id="toc-5">Computing gradients</h3>
<p><a href="https://en.wikipedia.org/wiki/Automatic_differentiation">Automatic differentiation</a> is useful for implementing machine learning algorithms such as <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> for training neural networks. During eager execution, use <code>tf.GradientTape</code> to trace operations for computing gradients later.</p>
<p><code>tf.GradientTape</code> is an opt-in feature to provide maximal performance when not tracing. Since different operations can occur during each call, all forward-pass operations get recorded to a "tape". To compute the gradient, play the tape backwards and then discard. A particular <code>tf.GradientTape</code> can only compute one gradient; subsequent calls throw a runtime error.</p>
<pre><code class="lang-py">w = tfe.Variable([[1.0]])
with tf.GradientTape() as tape:
  loss = w * w

grad = tape.gradient(loss, [w])
print(grad)  # =&gt; [tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)]
</code></pre>
<p>Here's an example of <code>tf.GradientTape</code> that records forward-pass operations to train a simple model:</p>
<pre><code class="lang-py"># A toy dataset of points around 3 * x + 2
NUM_EXAMPLES = 1000
training_inputs = tf.random_normal([NUM_EXAMPLES])
noise = tf.random_normal([NUM_EXAMPLES])
training_outputs = training_inputs * 3 + 2 + noise

def prediction(input, weight, bias):
  return input * weight + bias

# A loss function using mean-squared error
def loss(weights, biases):
  error = prediction(training_inputs, weights, biases) - training_outputs
  return tf.reduce_mean(tf.square(error))

# Return the derivative of loss with respect to weight and bias
def grad(weights, biases):
  with tf.GradientTape() as tape:
    loss_value = loss(weights, biases)
  return tape.gradient(loss_value, [weights, biases])

train_steps = 200
learning_rate = 0.01
# Start with arbitrary values for W and B on the same batch of data
W = tfe.Variable(5.)
B = tfe.Variable(10.)

print(&quot;Initial loss: {:.3f}&quot;.format(loss(W, B)))

for i in range(train_steps):
  dW, dB = grad(W, B)
  W.assign_sub(dW * learning_rate)
  B.assign_sub(dB * learning_rate)
  if i % 20 == 0:
    print(&quot;Loss at step {:03d}: {:.3f}&quot;.format(i, loss(W, B)))

print(&quot;Final loss: {:.3f}&quot;.format(loss(W, B)))
print(&quot;W = {}, B = {}&quot;.format(W.numpy(), B.numpy()))
</code></pre>
<p>Output (exact numbers may vary):</p>
<pre><code>Initial loss: 71.204
Loss at step 000: 68.333
Loss at step 020: 30.222
Loss at step 040: 13.691
Loss at step 060: 6.508
Loss at step 080: 3.382
Loss at step 100: 2.018
Loss at step 120: 1.422
Loss at step 140: 1.161
Loss at step 160: 1.046
Loss at step 180: 0.996
Final loss: 0.974
W = 3.01582956314, B = 2.1191945076
</code></pre>
<p>Replay the <code>tf.GradientTape</code> to compute the gradients and apply them in a training loop. This is demonstrated in an excerpt from the <a href="https://github.com/tensorflow/models/blob/master/official/mnist/mnist_eager.py">mnist_eager.py</a> example:</p>
<pre><code class="lang-py">dataset = tf.data.Dataset.from_tensor_slices((data.train.images,
                                              data.train.labels))
...
for (batch, (images, labels)) in enumerate(dataset):
  ...
  with tf.GradientTape() as tape:
    logits = model(images, training=True)
    loss_value = loss(logits, labels)
  ...
  grads = tape.gradient(loss_value, model.variables)
  optimizer.apply_gradients(zip(grads, model.variables),
                            global_step=tf.train.get_or_create_global_step())
</code></pre>
<p>The following example creates a multi-layer model that classifies the standard <a href="https://www.tensorflow.org/tutorials/layers">MNIST handwritten digits</a>. It demonstrates the optimizer and layer APIs to build trainable graphs in an eager execution environment.</p>
<h3 id="toc-6">Train a model</h3>
<p>Even without training, call the model and inspect the output in eager execution:</p>
<pre><code class="lang-py"># Create a tensor representing a blank image
batch = tf.zeros([1, 1, 784])
print(batch.shape)  # =&gt; (1, 1, 784)

result = model(batch)
# =&gt; tf.Tensor([[[ 0.  0., ..., 0.]]], shape=(1, 1, 10), dtype=float32)
</code></pre>
<p>This example uses the <a href="https://github.com/tensorflow/models/blob/master/official/mnist/dataset.py">dataset.py module</a> from the <a href="https://github.com/tensorflow/models/tree/master/official/mnist">TensorFlow MNIST example</a>; download this file to your local directory. Run the following to download the MNIST data files to your working directory and prepare a <code>tf.data.Dataset</code> for training:</p>
<pre><code class="lang-py">import dataset  # download dataset.py file
dataset_train = dataset.train(&#39;./datasets&#39;).shuffle(60000).repeat(4).batch(32)
</code></pre>
<p>To train a model, define a loss function to optimize and then calculate gradients. Use an optimizer to update the variables:</p>
<pre><code class="lang-py">def loss(model, x, y):
  prediction = model(x)
  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=prediction)

def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets)
  return tape.gradient(loss_value, model.variables)

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)

x, y = iter(dataset_train).next()
print(&quot;Initial loss: {:.3f}&quot;.format(loss(model, x, y)))

# Training loop
for (i, (x, y)) in enumerate(dataset_train):
  # Calculate derivatives of the input function with respect to its parameters.
  grads = grad(model, x, y)
  # Apply the gradient to the model
  optimizer.apply_gradients(zip(grads, model.variables),
                            global_step=tf.train.get_or_create_global_step())
  if i % 200 == 0:
    print(&quot;Loss at step {:04d}: {:.3f}&quot;.format(i, loss(model, x, y)))

print(&quot;Final loss: {:.3f}&quot;.format(loss(model, x, y)))
</code></pre>
<p>Output (exact numbers may vary):</p>
<pre><code>Initial loss: 2.674
Loss at step 0000: 2.593
Loss at step 0200: 2.143
Loss at step 0400: 2.009
Loss at step 0600: 2.103
Loss at step 0800: 1.621
Loss at step 1000: 1.695
...
Loss at step 6600: 0.602
Loss at step 6800: 0.557
Loss at step 7000: 0.499
Loss at step 7200: 0.744
Loss at step 7400: 0.681
Final loss: 0.670
</code></pre>
<p>And for faster training, move the computation to a GPU:</p>
<pre><code class="lang-py">with tf.device(&quot;/gpu:0&quot;):
  for (i, (x, y)) in enumerate(dataset_train):
    # minimize() is equivalent to the grad() and apply_gradients() calls.
    optimizer.minimize(lambda: loss(model, x, y),
                       global_step=tf.train.get_or_create_global_step())
</code></pre>
<h3 id="toc-7">Variables and optimizers</h3>
<p><code>tfe.Variable</code> objects store mutable <code>tf.Tensor</code> values accessed during training to make automatic differentiation easier. The parameters of a model can be encapsulated in classes as variables.</p>
<p>Better encapsulate model parameters by using <code>tfe.Variable</code> with <code>tf.GradientTape</code>. For example, the automatic differentiation example above can be rewritten:</p>
<pre><code class="lang-py">class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.W = tfe.Variable(5., name=&#39;weight&#39;)
    self.B = tfe.Variable(10., name=&#39;bias&#39;)
  def predict(self, inputs):
    return inputs * self.W + self.B

# A toy dataset of points around 3 * x + 2
NUM_EXAMPLES = 2000
training_inputs = tf.random_normal([NUM_EXAMPLES])
noise = tf.random_normal([NUM_EXAMPLES])
training_outputs = training_inputs * 3 + 2 + noise

# The loss function to be optimized
def loss(model, inputs, targets):
  error = model.predict(inputs) - targets
  return tf.reduce_mean(tf.square(error))

def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value = loss(model, inputs, targets)
  return tape.gradient(loss_value, [model.W, model.B])

# Define:
# 1. A model.
# 2. Derivatives of a loss function with respect to model parameters.
# 3. A strategy for updating the variables based on the derivatives.
model = Model()
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

print(&quot;Initial loss: {:.3f}&quot;.format(loss(model, training_inputs, training_outputs)))

# Training loop
for i in range(300):
  grads = grad(model, training_inputs, training_outputs)
  optimizer.apply_gradients(zip(grads, [model.W, model.B]),
                            global_step=tf.train.get_or_create_global_step())
  if i % 20 == 0:
    print(&quot;Loss at step {:03d}: {:.3f}&quot;.format(i, loss(model, training_inputs, training_outputs)))

print(&quot;Final loss: {:.3f}&quot;.format(loss(model, training_inputs, training_outputs)))
print(&quot;W = {}, B = {}&quot;.format(model.W.numpy(), model.B.numpy()))
</code></pre>
<p>Output (exact numbers may vary):</p>
<pre><code>Initial loss: 69.066
Loss at step 000: 66.368
Loss at step 020: 30.107
Loss at step 040: 13.959
Loss at step 060: 6.769
Loss at step 080: 3.567
Loss at step 100: 2.141
Loss at step 120: 1.506
Loss at step 140: 1.223
Loss at step 160: 1.097
Loss at step 180: 1.041
Loss at step 200: 1.016
Loss at step 220: 1.005
Loss at step 240: 1.000
Loss at step 260: 0.998
Loss at step 280: 0.997
Final loss: 0.996
W = 2.99431324005, B = 2.02129220963
</code></pre>
<h2 id="toc-8">Use objects for state during eager execution</h2>
<p>With graph execution, program state (such as the variables) is stored in global collections and their lifetime is managed by the <code>tf.Session</code> object. In contrast, during eager execution the lifetime of state objects is determined by the lifetime of their corresponding Python object.</p>
<h3 id="toc-9">Variables are objects</h3>
<p>During eager execution, variables persist until the last reference to the object is removed, and is then deleted.</p>
<pre><code class="lang-py">with tf.device(&quot;gpu:0&quot;):
  v = tfe.Variable(tf.random_normal([1000, 1000]))
  v = None  # v no longer takes up GPU memory
</code></pre>
<h3 id="toc-10">Object-based saving</h3>
<p><code>tfe.Checkpoint</code> can save and restore <code>tfe.Variable</code>s to and from checkpoints:</p>
<pre><code class="lang-py">x = tfe.Variable(10.)

checkpoint = tfe.Checkpoint(x=x)  # save as &quot;x&quot;

x.assign(2.)   # Assign a new value to the variables and save.
save_path = checkpoint.save(&#39;./ckpt/&#39;)

x.assign(11.)  # Change the variable after saving.

# Restore values from the checkpoint
checkpoint.restore(save_path)

print(x)  # =&gt; 2.0
</code></pre>
<p>To save and load models, <code>tfe.Checkpoint</code> stores the internal state of objects, without requiring hidden variables. To record the state of a <code>model</code>, an <code>optimizer</code>, and a global step, pass them to a <code>tfe.Checkpoint</code>:</p>
<pre><code class="lang-py">model = MyModel()
optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
checkpoint_dir = ‘/path/to/model_dir’
checkpoint_prefix = os.path.join(checkpoint_dir, &quot;ckpt&quot;)
root = tfe.Checkpoint(optimizer=optimizer,
                      model=model,
                      optimizer_step=tf.train.get_or_create_global_step())

root.save(file_prefix=checkpoint_prefix)
# or
root.restore(tf.train.latest_checkpoint(checkpoint_dir))
</code></pre>
<h3 id="toc-11">Object-oriented metrics</h3>
<p><code>tfe.metrics</code> are stored as objects. Update a metric by passing the new data to the callable, and retrieve the result using the <code>tfe.metrics.result</code> method, for example:</p>
<pre><code class="lang-py">m = tfe.metrics.Mean(&quot;loss&quot;)
m(0)
m(5)
m.result()  # =&gt; 2.5
m([8, 9])
m.result()  # =&gt; 5.5
</code></pre>
<h4 id="toc-12">Summaries and TensorBoard</h4>
<p><a href="//tensorflow.juejin.im/programmers_guide/summaries_and_tensorboard.html">Tensorboard：可视化学习面板</a></p>
<p><code>tf.contrib.summary</code> is compatible with both eager and graph execution environments. Summary operations, such as <code>tf.contrib.summary.scalar</code>, are inserted during model construction. For example, to record summaries once every 100 global steps:</p>
<pre><code class="lang-py">writer = tf.contrib.summary.create_file_writer(logdir)
global_step=tf.train.get_or_create_global_step()  # return global step var

writer.set_as_default()

for _ in range(iterations):
  global_step.assign_add(1)
  # Must include a record_summaries method
  with tf.contrib.summary.record_summaries_every_n_global_steps(100):
    # your model code goes here
    tf.contrib.summary.scalar(&#39;loss&#39;, loss)
     ...
</code></pre>
<h2 id="toc-13">Advanced automatic differentiation topics</h2>
<h3 id="toc-14">Dynamic models</h3>
<p><code>tf.GradientTape</code> can also be used in dynamic models. This example for a <a href="https://wikipedia.org/wiki/Backtracking_line_search">backtracking line search</a> algorithm looks like normal NumPy code, except there are gradients and is differentiable, despite the complex control flow:</p>
<pre><code class="lang-py">def line_search_step(fn, init_x, rate=1.0):
  with tf.GradientTape() as tape:
    # Variables are automatically recorded, but manually watch a tensor
    tape.watch(init_x)
    value = fn(init_x)
  grad, = tape.gradient(value, [init_x])
  grad_norm = tf.reduce_sum(grad * grad)
  init_value = value
  while value &gt; init_value - rate * grad_norm:
    x = init_x - rate * grad
    value = fn(x)
    rate /= 2.0
  return x, value
</code></pre>
<h3 id="toc-15">Additional functions to compute gradients</h3>
<p><code>tf.GradientTape</code> is a powerful interface for computing gradients, but there is another <a href="https://github.com/HIPS/autograd">Autograd</a>-style API available for automatic differentiation. These functions are useful if writing math code with only tensors and gradient functions, and without <code>tfe.Variables</code>:</p>
<ul>
<li><code>tfe.gradients_function</code> —Returns a function that computes the derivatives of its input function parameter with respect to its arguments. The input function parameter must return a scalar value. When the returned function is invoked, it returns a list of <code>tf.Tensor</code> objects: one element for each argument of the input function. Since anything of interest must be passed as a function parameter, this becomes unwieldy if there's a dependency on many trainable parameters.</li>
<li><code>tfe.value_and_gradients_function</code> —Similar to <code>tfe.gradients_function</code>, but when the returned function is invoked, it returns the value from the input function in addition to the list of derivatives of the input function with respect to its arguments.</li>
</ul>
<p>In the following example, <code>tfe.gradients_function</code> takes the <code>square</code> function as an argument and returns a function that computes the partial derivatives of <code>square</code> with respect to its inputs. To calculate the derivative of <code>square</code> at <code>3</code>, <code>grad(3.0)</code> returns <code>6</code>.</p>
<pre><code class="lang-py">def square(x):
  return tf.multiply(x, x)

grad = tfe.gradients_function(square)

square(3.)  # =&gt; 9.0
grad(3.)    # =&gt; [6.0]

# The second-order derivative of square:
gradgrad = tfe.gradients_function(lambda x: grad(x)[0])
gradgrad(3.)  # =&gt; [2.0]

# The third-order derivative is None:
gradgradgrad = tfe.gradients_function(lambda x: gradgrad(x)[0])
gradgradgrad(3.)  # =&gt; [None]


# With flow control:
def abs(x):
  return x if x &gt; 0. else -x

grad = tfe.gradients_function(abs)

grad(3.)   # =&gt; [1.0]
grad(-3.)  # =&gt; [-1.0]
</code></pre>
<h3 id="toc-16">Custom gradients</h3>
<p>Custom gradients are an easy way to override gradients in eager and graph execution. Within the forward function, define the gradient with respect to the inputs, outputs, or intermediate results. For example, here's an easy way to clip the norm of the gradients in the backward pass:</p>
<pre><code class="lang-py">@tf.custom_gradient
def clip_gradient_by_norm(x, norm):
  y = tf.identity(x)
  def grad_fn(dresult):
    return [tf.clip_by_norm(dresult, norm), None]
  return y, grad_fn
</code></pre>
<p>Custom gradients are commonly used to provide a numerically stable gradient for a sequence of operations:</p>
<pre><code class="lang-py">def log1pexp(x):
  return tf.log(1 + tf.exp(x))
grad_log1pexp = tfe.gradients_function(log1pexp)

# The gradient computation works fine at x = 0.
grad_log1pexp(0.)  # =&gt; [0.5]

# However, x = 100 fails because of numerical instability.
grad_log1pexp(100.)  # =&gt; [nan]
</code></pre>
<p>Here, the <code>log1pexp</code> function can be analytically simplified with a custom gradient. The implementation below reuses the value for <code>tf.exp(x)</code> that is computed during the forward pass—making it more efficient by eliminating redundant calculations:</p>
<pre><code class="lang-py">@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.log(1 + e), grad

grad_log1pexp = tfe.gradients_function(log1pexp)

# As before, the gradient computation works fine at x = 0.
grad_log1pexp(0.)  # =&gt; [0.5]

# And the gradient computation also works at x = 100.
grad_log1pexp(100.)  # =&gt; [1.0]
</code></pre>
<h2 id="toc-17">Performance</h2>
<p>Computation is automatically offloaded to GPUs during eager execution. If you want control over where a computation runs you can enclose it in a <code>tf.device('/gpu:0')</code> block (or the CPU equivalent):</p>
<pre><code class="lang-py">import time

def measure(x, steps):
  # TensorFlow initializes a GPU the first time it&#39;s used, exclude from timing.
  tf.matmul(x, x)
  start = time.time()
  for i in range(steps):
    x = tf.matmul(x, x)
    _ = x.numpy()  # Make sure to execute op and not just enqueue it
  end = time.time()
  return end - start

shape = (1000, 1000)
steps = 200
print(&quot;Time to multiply a {} matrix by itself {} times:&quot;.format(shape, steps))

# Run on CPU:
with tf.device(&quot;/cpu:0&quot;):
  print(&quot;CPU: {} secs&quot;.format(measure(tf.random_normal(shape), steps)))

# Run on GPU, if available:
if tfe.num_gpus() &gt; 0:
  with tf.device(&quot;/gpu:0&quot;):
    print(&quot;GPU: {} secs&quot;.format(measure(tf.random_normal(shape), steps)))
else:
  print(&quot;GPU: not found&quot;)
</code></pre>
<p>Output (exact numbers depend on hardware):</p>
<pre><code>Time to multiply a (1000, 1000) matrix by itself 200 times:
CPU: 4.614904403686523 secs
GPU: 0.5581181049346924 secs
</code></pre>
<p>A <code>tf.Tensor</code> object can be copied to a different device to execute its operations:</p>
<pre><code class="lang-py">x = tf.random_normal([10, 10])

x_gpu0 = x.gpu()
x_cpu = x.cpu()

_ = tf.matmul(x_cpu, x_cpu)    # Runs on CPU
_ = tf.matmul(x_gpu0, x_gpu0)  # Runs on GPU:0

if tfe.num_gpus() &gt; 1:
  x_gpu1 = x.gpu(1)
  _ = tf.matmul(x_gpu1, x_gpu1)  # Runs on GPU:1
</code></pre>
<h3 id="toc-18">Benchmarks</h3>
<p>For compute-heavy models, such as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/resnet50">ResNet50</a> training on a GPU, eager execution performance is comparable to graph execution. But this gap grows larger for models with less computation and there is work to be done for optimizing hot code paths for models with lots of small operations.</p>
<h2 id="toc-19">Work with graphs</h2>
<p>While eager execution makes development and debugging more interactive, TensorFlow graph execution has advantages for distributed training, performance optimizations, and production deployment. However, writing graph code can feel different than writing regular Python code and more difficult to debug.</p>
<p>For building and training graph-constructed models, the Python program first builds a graph representing the computation, then invokes <code>Session.run</code> to send the graph for execution on the C++-based runtime.  This provides:</p>
<ul>
<li>Automatic differentiation using static autodiff.</li>
<li>Simple deployment to a platform independent server.</li>
<li>Graph-based optimizations (common subexpression elimination, constant-folding, etc.).</li>
<li>Compilation and kernel fusion.</li>
<li>Automatic distribution and replication (placing nodes on the distributed system).</li>
</ul>
<p>Deploying code written for eager execution is more difficult: either generate a graph from the model, or run the Python runtime and code directly on the server.</p>
<h3 id="toc-20">Write compatible code</h3>
<p>The same code written for eager execution will also build a graph during graph execution. Do this by simply running the same code in a new Python session where eager execution is not enabled.</p>
<p>Most TensorFlow operations work during eager execution, but there are some things to keep in mind:</p>
<ul>
<li>Use <code>tf.data</code> for input processing instead of queues. It's faster and easier.</li>
<li>Use object-oriented layer APIs—like <code>tf.keras.layers</code> and <code>tf.keras.Model</code>—since they have explicit storage for variables.</li>
<li>Most model code works the same during eager and graph execution, but there are exceptions. (For example, dynamic models using Python control flow to change the computation based on inputs.)</li>
<li>Once eager execution is enabled with <code>tf.enable_eager_execution</code>, it cannot be turned off. Start a new Python session to return to graph execution.</li>
</ul>
<p>It's best to write code for both eager execution <em>and</em> graph execution. This gives you eager's interactive experimentation and debuggability with the distributed performance benefits of graph execution.</p>
<p>Write, debug, and iterate in eager execution, then import the model graph for production deployment. Use <code>tfe.Checkpoint</code> to save and restore model variables, this allows movement between eager and graph execution environments. See the examples in: <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples">tensorflow/contrib/eager/python/examples</a>.</p>
<h3 id="toc-21">Use eager execution in a graph environment</h3>
<p>Selectively enable eager execution in a TensorFlow graph environment using <code>tfe.py_func</code>. This is used when <code>tf.enable_eager_execution()</code> has <em>not</em> been called.</p>
<pre><code class="lang-py">def my_py_func(x):
  x = tf.matmul(x, x)  # You can use tf ops
  print(x)  # but it&#39;s eager!
  return x

with tf.Session() as sess:
  x = tf.placeholder(dtype=tf.float32)
  # Call eager function in graph!
  pf = tfe.py_func(my_py_func, [x], tf.float32)
  sess.run(pf, feed_dict={x: [[2.0]]})  # [[4.0]]
</code></pre>

        </main>
        <div class="d-none d-xl-block col-xl-2 bd-toc">
        <ul id="table-of-content">
<li><a href="#toc-0">Eager Execution</a><ul>
<li><a href="#toc-1">Setup and basic usage</a></li>
<li><a href="#toc-2">Dynamic control flow</a></li>
<li><a href="#toc-3">Build a model</a></li>
<li><a href="#toc-4">Eager training</a></li>
<li><a href="#toc-8">Use objects for state during eager execution</a></li>
<li><a href="#toc-13">Advanced automatic differentiation topics</a></li>
<li><a href="#toc-17">Performance</a></li>
<li><a href="#toc-19">Work with graphs</a></li>
</ul>
</li>
</ul>

        </div>
    </div>
</div>
<!-- Content end-->

<!-- Footer start -->
<footer class="footer">
    <div class="container">
        <div>如果您发现本页面存在错误或可以改进，请<a href="https://github.com/xitu/tensorflow-docs/blob/zh-hans/programmers_guide/eager.md" target="_blank">点击此处</a>帮助我们改进。本页贡献者：<span id="contributors"></span></div>
        <hr/>
        <div class="text-center official-links">
            <a href="https://www.tensorflow.org"><img
                    src="https://www.tensorflow.org/_static/b1fb9a8564/images/tensorflow/lockup.png" height="20"/></a>
            <a href="https://github.com/xitu/tensorflow-docs"><img
                    src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Logo.png" height="20"></a>
            <a href="https://juejin.im"><img src="//tensorflow.juejin.im/assets/imgs/logo_app_white.png" height="20"/></a>
        </div>
    </div>
</footer>
<script>
    var contributors = [{'leviding': 'https://avatars3.githubusercontent.com/u/26959437?v=4'}]
</script>
<!-- Footer end -->
</body>
<script src="//tensorflow.juejin.im/assets/js/jquery.slim.min.js" type="text/javascript"></script>
<script src="//tensorflow.juejin.im/assets/js/highlight.min.js" type="text/javascript"></script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
<script src="//tensorflow.juejin.im/assets/js/main.js" type="text/javascript"></script>
</html>