<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>为 XLA 开发一个新后端</title>
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/main.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
</head>
<body>
<!-- Header start -->
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="#">TensorFlow</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
        </ul>
        <form class="form-inline my-2 my-lg-0">
            <input class="form-control mr-sm-2" type="search" placeholder="Search" aria-label="Search">
            <button class="btn btn-outline-success my-2 my-sm-0" type="submit">Search</button>
        </form>
    </div>
</nav>
<script>
    var head = [{'link': '//xitu.github.io/tensorflow-docs-web/extend/index.html', 'name': '扩展', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/install/index.html', 'name': '安装 TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/deploy/index.html', 'name': '部署', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/about/index.html', 'name': 'About TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/index.html', 'name': '开始', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/index.html', 'name': 'Overview', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/tutorials/index.html', 'name': '教程', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/index.html', 'name': '性能', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/community/index.html', 'name': 'Community', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/programmers_guide/index.html', 'name': '开发者指南', 'selected': 0}]
</script>
<!-- Header end -->

<!-- Content start-->
<div class="container-fluid">
    <div class="row">
        
        <main role="main" class="col-md-9 ml-sm-auto col-lg-10 pt-3 px-4">
            <h1>为 XLA 开发一个新后端</h1>
<p>本初级指南针对的是早期用户，他们希望用一种高效的方式轻易地将 TensorFlow 重定向到他们自己的硬件上。本指南不会手把手地讲解，我们假设读者已经了解 <a href="http://llvm.org">LLVM</a>、<a href="https://bazel.build/">Bazel</a>、以及 TensorFlow。</p>
<p>XLA 提供了一个抽象的接口，让新的架构或加速器可以实现并创建可运行 TensorFlow 计算图的后端。<br>
在新的硬件上重定向 XLA 要比重新实现 TensorFlow 所有的操作更简单、更具扩展性。</p>
<p>大部分实现都属于下列情形之一：</p>
<ol>
<li>已有的 CPU 架构尚没有在官方 XLA 中得到支持，存在或不存在已有的 <a href="http://llvm.org">LLVM</a> 后端。</li>
<li>非 CPU 硬件，已有 LLVM 后端。</li>
<li>非 CPU 硬件，没有 LLVM 后端。</li>
</ol>
<blockquote><p>注意：LLVM 后端既包括官方发布的 LLVM 后端，也包括内部开发的定制 LLVM 后端。</p>
</blockquote>
<h2>场景 1：已有的 CPU 架构，但官方 XLA 尚不支持</h2>
<p>在此场景中，先查看一下已有的 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/cpu/">XLA CPU 后端</a>。通过 LLVM，XLA 让 TensorFlow 更容易重定向到不同的 CPU，因为对于 CPU 来说， XLA 后端之间的主要区别在于 LLVM 生成的代码。Google 在 x64 和 ARM64 架构下测试了 XLA。</p>
<p>如果硬件厂商已经为他们的硬件开发了 LLVM 后端，将此后端与用 XLA 构建的 LLVM 链接起来是比较容易的。<br>
在 JIT 模式中，XLA CPU 后端为主机 CPU 生成代码。对于提前（ahead-of-time）编译，<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/compiler.h"><code>xla::AotCompilationOptions</code></a> 可提供一个 LLVM 三元组，来配置目标架构。</p>
<p>如果尚不存在 LLVM 后端，但是存在另一种代码生成器，重用已有 CPU 后端的代码也是可能的。</p>
<h2>场景 2：非 CPU 硬件，已有 LLVM 后端</h2>
<p>在已有的<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc"><code>xla::CPUCompiler</code></a><br>
和 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/gpu/gpu_compiler.cc"><code>xla::GPUCompiler</code></a> 类的基础上，是有可能建模出一个新的<br>
<a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/compiler.h"><code>xla::Compiler</code></a> 实现的。因为它们已经生成了 LLVM IR。<br>
根据硬件的不同，有可能许多 LLVM IR 生成的过程也会不同，但是很多代码是可以和已有的后端共享的。</p>
<p>参考的一个很好的例子是 XLA 的 <a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/gpu/">GPU 后端</a>。<br>
这个 GPU 后端针对的非 CPU 的 ISA，因而它的代码生成的很多方面是 GPU 领域专有的。其它类型的硬件，比如，DSP 一类的 Hexagon （它们是有上游的 LLVM 后端的），可以重用 LLVM IR 生成逻辑的部分代码，但是其它部分则是不一样的。</p>
<h2>场景 3：非 CPU 硬件，没有 LLVM 后端</h2>
<p>如果没有可能利用 LLVM，则最好的选择是为你的硬件及 XLA 实现一个全新的后端。<br>
这是最难的选择，因为你要实现如下的类：</p>
<ul>
<li><a href="https://www.tensorflow.org/code/tensorflow/stream_executor/stream_executor.h"><code>StreamExecutor</code></a>:<br>
对于许多设备而言，不是 <code>StreamExecutor</code> 中的所有方法都是必要的。更多细节，参见已有 <code>StreamExecutor</code> 的实现。</li>
<li><a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/compiler.h"><code>xla::Compiler</code></a>:<br>
这个类封装了一个 HLO 计算到一个 <code>xla::Executable</code> 的编译。</li>
<li><a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/executable.h"><code>xla::Executable</code></a>:<br>
   这个类用于在目标平台上启动一个编译后的计算过程。</li>
<li><a href="https://www.tensorflow.org/code/tensorflow/compiler/xla/service/transfer_manager.h"><code>xla::TransferManager</code></a>:<br>
   这个类让后端提供针对目标平台的机制，用于从设备内存句柄构造出 XLA 字面量数据。换句话说，它帮助封装了主 CPU 与设备之间的数据传输。</li>
</ul>

        </main>
    </div>
</div>
<!-- Content end-->
</body>
<script src="//cdn.bootcss.com/jquery/3.3.1/jquery.slim.min.js" type="text/javascript"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js" type="text/javascript"></script>
<script src="//xitu.github.io/tensorflow-docs-web/assets/js/main.js" type="text/javascript"></script>
</html>