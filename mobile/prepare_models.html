<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>Preparing models for mobile deployment</title>
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/main.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
</head>
<body>
<!-- Header start -->
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="#">TensorFlow</a>
    <button class="navbar-toggler" type="button" aria-expanded="false" aria-label="Menu"
            onclick="$('.collapse').toggle()">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse">
        <ul class="navbar-nav mr-auto">
        </ul>
        <!-- TODO: Search function-->
        <!--<form class="form-inline my-2 my-lg-0">-->
            <!--<input class="form-control mr-sm-2" type="search" placeholder="Search" aria-label="Search">-->
            <!--<button class="btn btn-outline-success my-2 my-sm-0" type="submit">Search</button>-->
        <!--</form>-->
    </div>
</nav>
<script>
    var head = [{'link': '//xitu.github.io/tensorflow-docs-web/extend/index.html', 'name': '扩展', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/install/index.html', 'name': '安装 TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/deploy/index.html', 'name': '部署', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/about/index.html', 'name': 'About TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/index.html', 'name': '开始', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/index.html', 'name': '概述', 'selected': 1}, {'link': '//xitu.github.io/tensorflow-docs-web/tutorials/index.html', 'name': '教程', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/javascript/index.html', 'name': 'JavaScript', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/index.html', 'name': '性能', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/community/index.html', 'name': 'Community', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/programmers_guide/index.html', 'name': '开发者指南', 'selected': 0}]
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-92630037-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-92630037-8');
</script>

<!-- Header end -->

<!-- Content start-->
<div class="container-fluid">
    <div class="row">
        <nav class="col-md-2 d-none d-md-block bg-light sidebar">
    <div class="sidebar-sticky" id="left-nav">

    </div>
</nav>
<script>
    var nav = [{'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/mobile/index.html', 'title': '概述'}, {'type': 'parent', 'title': ' TensorFlow Lite', 'sub_class': [{'link': '//xitu.github.io/tensorflow-docs-web/mobile/tflite/index.html', 'title': 'TensorFlow Lite 简介'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/tflite/devguide.html', 'title': 'Developer Guide'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/tflite/demo_android.html', 'title': 'Android 示例应用'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/tflite/demo_ios.html', 'title': 'iOS 演示 APP'}]}, {'type': 'parent', 'title': ' TensorFlow Mobile', 'sub_class': [{'link': '//xitu.github.io/tensorflow-docs-web/mobile/mobile_intro.html', 'title': 'TensorFlow Mobile 简介'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/android_build.html', 'title': '在安卓上构建 TensorFlow'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/ios_build.html', 'title': '在 iOS 中构建 TensorFlow'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/linking_libs.html', 'title': '集成 TensorFlow 库'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/prepare_models.html', 'title': 'Preparing models for mobile deployment'}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/optimizing.html', 'title': '移动端优化'}]}]
</script>
        <main role="main" class="col-md-9 ml-sm-auto col-lg-10 pt-3 px-4">
            <h1>Preparing models for mobile deployment</h1>
<p>The requirements for storing model information during training are very different from when you want to release it as part of a mobile app. This section covers the tools involved in converting from a training model to something releasable in production.</p>
<h2>What is up with all the different saved file formats?</h2>
<p>You may find yourself getting very confused by all the different ways that TensorFlow can save out graphs. To help, here’s a rundown of some of the different components, and what they are used for. The objects are mostly defined and serialized as protocol buffers:</p>
<ul>
<li><p><a href="https://www.tensorflow.org/code/tensorflow/core/framework/node_def.proto">NodeDef</a>: Defines a single operation in a model. It has a unique name, a list of the names of other nodes it pulls inputs from, the operation type it implements (for example <code>Add</code>, or <code>Mul</code>), and any attributes that are needed to control that operation. This is the basic unit of computation for TensorFlow, and all work is done by iterating through a network of these nodes, applying each one in turn. One particular operation type that’s worth knowing about is <code>Const</code>, since this holds information about a constant. This may be a single, scalar number or string, but it can also hold an entire multi-dimensional tensor array. The values for a <code>Const</code> are stored inside the <code>NodeDef</code>, and so large constants can take up a lot of room when serialized.</p>
</li>
<li><p><a href="https://www.tensorflow.org/code/tensorflow/core/util/tensor_bundle/tensor_bundle.h">Checkpoint</a>. Another way of storing values for a model is by using <code>Variable</code> ops. Unlike <code>Const</code> ops, these don’t store their content as part of the <code>NodeDef</code>, so they take up very little space within the <code>GraphDef</code> file. Instead their values are held in RAM while a computation is running, and then saved out to disk as checkpoint files periodically. This typically happens as a neural network is being trained and weights are updated, so it’s a time-critical operation, and it may happen in a distributed fashion across many workers, so the file format has to be both fast and flexible. They are stored as multiple checkpoint files, together with metadata files that describe what’s contained within the checkpoints. When you’re referring to a checkpoint in the API (for example when passing a filename in as a command line argument), you’ll use the common prefix for a set of related files. If you had these files:</p>
<pre><code>  /tmp/model/model-chkpt-1000.data-00000-of-00002
  /tmp/model/model-chkpt-1000.data-00001-of-00002
  /tmp/model/model-chkpt-1000.index
  /tmp/model/model-chkpt-1000.meta
</code></pre>
<p>You would refer to them as <code>/tmp/model/chkpt-1000</code>.</p>
</li>
<li><p><a href="https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto">GraphDef</a>: Has a list of <code>NodeDefs</code>, which together define the computational graph to execute. During training, some of these nodes will be <code>Variables</code>, and so if you want to have a complete graph you can run, including the weights, you’ll need to call a restore operation to pull those values from checkpoints. Because checkpoint loading has to be flexible to deal with all of the training requirements, this can be tricky to implement on mobile and embedded devices, especially those with no proper file system available like iOS. This is where the <a href="https://www.tensorflow.org/code/tensorflow/python/tools/freeze_graph.py"><code>freeze_graph.py</code></a> script comes in handy. As mentioned above, <code>Const</code> ops store their values as part of the <code>NodeDef</code>, so if all the <code>Variable</code> weights are converted to <code>Const</code> nodes, then we only need a single <code>GraphDef</code> file to hold the model architecture and the weights. Freezing the graph handles the process of loading the checkpoints, and then converts all Variables to Consts. You can then load the resulting file in a single call, without having to restore variable values from checkpoints. One thing to watch out for with <code>GraphDef</code> files is that sometimes they’re stored in text format for easy inspection. These versions usually have a ‘.pbtxt’ filename suffix, whereas the binary files end with ‘.pb’.</p>
</li>
<li><p><a href="https://www.tensorflow.org/code/tensorflow/core/framework/function.proto">FunctionDefLibrary</a>: This appears in <code>GraphDef</code>, and is effectively a set of sub-graphs, each with information about their input and output nodes. Each sub-graph can then be used as an op in the main graph, allowing easy instantiation of different nodes, in a similar way to how functions encapsulate code in other languages.</p>
</li>
<li><p><a href="https://www.tensorflow.org/code/tensorflow/core/protobuf/meta_graph.proto">MetaGraphDef</a>: A plain <code>GraphDef</code> only has information about the network of computations, but doesn’t have any extra information about the model or how it can be used. <code>MetaGraphDef</code> contains a <code>GraphDef</code> defining the computation part of the model, but also includes information like ‘signatures’, which are suggestions about which inputs and outputs you may want to call the model with, data on how and where any checkpoint files are saved, and convenience tags for grouping ops together for ease of use.</p>
</li>
<li><p><a href="https://www.tensorflow.org/code/tensorflow/core/protobuf/saved_model.proto">SavedModel</a>: It’s common to want to have different versions of a graph that rely on a common set of variable checkpoints. For example, you might need a GPU and a<br>
CPU version of the same graph, but keep the same weights for both. You might also need some extra files (like label names) as part of your model. The <a href="https://www.tensorflow.org/code/tensorflow/python/saved_model/README.md">SavedModel</a> format addresses these needs by letting you save multiple versions of the same graph without duplicating variables, and also storing asset files in the same bundle. Under the hood, it uses <code>MetaGraphDef</code> and checkpoint files, along<br>
with extra metadata files. It’s the format that you’ll want to use if you’re deploying a web API using TensorFlow Serving, for example.</p>
</li>
</ul>
<h2>How do you get a model you can use on mobile?</h2>
<p>In most situations, training a model with TensorFlow will give you a folder ontaining a <code>GraphDef</code> file (usually ending with the <code>.pb</code> or <code>.pbtxt</code> extension) and a set of checkpoint files. What you need for mobile or embedded deployment is a single <code>GraphDef</code> file that’s been ‘frozen’, or had its variables converted into inline constants so everything’s in one file.  To handle the conversion, you’ll need the <code>freeze_graph.py</code> script, that’s held in <a href="https://www.tensorflow.org/code/tensorflow/python/tools/freeze_graph.py"><code>tensorflow/python/tools/freeze_graph.py</code></a>. You’ll run it like this:</p>
<pre><code>bazel build tensorflow/tools:freeze_graph
bazel-bin/tensorflow/tools/freeze_graph \
--input_graph=/tmp/model/my_graph.pb \
--input_checkpoint=/tmp/model/model.ckpt-1000 \
--output_graph=/tmp/frozen_graph.pb \
--output_node_names=output_node \
</code></pre>
<p>The <code>input_graph</code> argument should point to the <code>GraphDef</code> file that holds your model architecture. It’s possible that your <code>GraphDef</code> has been stored in a text format on disk, in which case it’s likely to end in <code>.pbtxt</code> instead of <code>.pb</code>, and you should add an extra <code>--input_binary=false</code> flag to the command.</p>
<p>The <code>input_checkpoint</code> should be the most recent saved checkpoint. As mentioned in the checkpoint section, you need to give the common prefix to the set of checkpoints here, rather than a full filename.</p>
<p><code>output_graph</code> defines where the resulting frozen <code>GraphDef</code> will be saved. Because it’s likely to contain a lot of weight values that take up a large amount of space in text format, it’s always saved as a binary protobuf.</p>
<p><code>output_node_names</code> is a list of the names of the nodes that you want to extract the results of your graph from. This is needed because the freezing process needs to understand which parts of the graph are actually needed, and which are artifacts of the training process, like summarization ops. Only ops that contribute to calculating the given output nodes will be kept. If you know how your graph is going to be used, these should just be the names of the nodes you pass into <code>Session::Run()</code> as your fetch targets. The easiest way to find the node names is to inspect the Node objects while building your graph in python. Inspecting your graph in TensorBoard is another simple way.  You can get some suggestions on likely outputs by running the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms/README.md#inspecting-graphs"><code>summarize_graph</code> tool</a>.</p>
<p>Because the output format for TensorFlow has changed over time, there are a variety of other less commonly used flags available too, like <code>input_saver</code>, but hopefully you shouldn’t need these on graphs trained with modern versions of the framework.</p>
<h2>Using the Graph Transform Tool</h2>
<p>A lot of the things you need to do to efficiently run a model on device are available through the <a href="https://www.tensorflow.org/code/tensorflow/tools/graph_transforms/README.md">Graph Transform Tool</a>. This command-line tool takes an input <code>GraphDef</code> file, applies the set of rewriting rules you request, and then writes out the result as a <code>GraphDef</code>. See the documentation for more information on how to build and run this tool.</p>
<h3>Removing training-only nodes</h3>
<p>TensorFlow <code>GraphDefs</code> produced by the training code contain all of the computation that’s needed for back-propagation and updates of weights, as well as the queuing and decoding of inputs, and the saving out of checkpoints. All of these nodes are no longer needed during inference, and some of the operations like checkpoint saving aren’t even supported on mobile platforms. To create a model file that you can load on devices you need to delete those unneeded operations by running the <code>strip_unused_nodes</code> rule in the Graph Transform Tool.</p>
<p>The trickiest part of this process is figuring out the names of the nodes you want to use as inputs and outputs during inference.  You'll need these anyway once you start to run inference, but you also need them here so that the transform can calculate which nodes are not needed on the inference-only path. These may not be obvious from the training code. The easiest way to determine the node name is to explore the graph with TensorBoard.</p>
<p>Remember that mobile applications typically gather their data from sensors and have it as arrays in memory, whereas training typically involves loading and decoding representations of the data stored on disk. In the case of Inception v3 for example, there’s a <code>DecodeJpeg</code> op at the start of the graph that’s designed to take JPEG-encoded data from a file retrieved from disk and turn it into an arbitrary-sized image. After that there’s a <code>BilinearResize</code> op to scale it to the expected size, followed by a couple of other ops that convert the byte data into float and scale the value magnitudes it in the way the rest of the graph expects. A typical mobile app will skip most of these steps because it’s getting its input directly from a live camera, so the input node you will actually supply will be the output of the <code>Mul</code> node in this case.</p>
<p>&lt;img src ="../images/inception_input.png" width="300"&gt;</p>
<p>You’ll need to do a similar process of inspection to figure out the correct utput nodes.</p>
<p>If you’ve just been given a frozen <code>GraphDef</code> file, and are not sure about the contents, try using the <code>summarize_graph</code> tool to print out information bout the inputs and outputs it finds from the graph structure. Here’s an xample with the original Inception v3 file:</p>
<pre><code>bazel run tensorflow/tools/graph_transforms:summarize_graph --
--in_graph=tensorflow_inception_graph.pb
</code></pre>
<p>Once you have an idea of what the input and output nodes are, you can feed them into the graph transform tool as the <code>--input_names</code> and <code>--output_names</code> arguments, and call the <code>strip_unused_nodes</code> transform, like this:</p>
<pre><code>bazel run tensorflow/tools/graph_transforms:transform_graph --
--in_graph=tensorflow_inception_graph.pb
--out_graph=optimized_inception_graph.pb --inputs='Mul' --outputs='softmax'
--transforms='
  strip_unused_nodes(type=float, shape="1,299,299,3")
  fold_constants(ignore_errors=true)
  fold_batch_norms
  fold_old_batch_norms'
</code></pre>
<p>One thing to look out for here is that you need to specify the size and type that you want your inputs to be. This is because any values that you’re going to be passing in as inputs to inference need to be fed to special <code>Placeholder</code> op nodes, and the transform may need to create them if they don’t already exist. In the case of Inception v3 for example, a <code>Placeholder</code> node replaces the old <code>Mul</code> node that used to output the resized and rescaled image array, since we’re going to be doing that processing ourselves before we call TensorFlow. It keeps the original name though, which is why we always feed in inputs to <code>Mul</code> when we run a session with our modified Inception graph.</p>
<p>After you’ve run this process, you’ll have a graph that only contains the actual nodes you need to run your prediction process. This is the point where it becomes useful to run metrics on the graph, so it’s worth running <code>summarize_graph</code> again to understand what’s in your model.</p>
<h2>What ops should you include on mobile?</h2>
<p>There are hundreds of operations available in TensorFlow, and each one has multiple implementations for different data types. On mobile platforms, the size of the executable binary that’s produced after compilation is important, because app download bundles need to be as small as possible for the best user experience. If all of the ops and data types are compiled into the TensorFlow library then the total size of the compiled library can be tens of megabytes, so by default only a subset of ops and data types are included.</p>
<p>That means that if you load a model file that’s been trained on a desktop machine, you may see the error “No OpKernel was registered to support Op” when you load it on mobile. The first thing to try is to make sure you’ve stripped out any training-only nodes, since the error will occur at load time even if the op is never executed. If you’re still hitting the same problem once that’s done, you’ll need to look at adding the op to your built library.</p>
<p>The criteria for including ops and types fall into several categories:</p>
<ul>
<li><p>Are they only useful in back-propagation, for gradients? Since mobile is focused on inference, we don’t include these.</p>
</li>
<li><p>Are they useful mainly for other training needs, such as checkpoint saving? These we leave out.</p>
</li>
<li><p>Do they rely on frameworks that aren’t always available on mobile, such as libjpeg? To avoid extra dependencies we don’t include ops like <code>DecodeJpeg</code>.</p>
</li>
<li><p>Are there types that aren’t commonly used? We don’t include boolean variants of ops for example, since we don’t see much use of them in typical inference graphs.</p>
</li>
</ul>
<p><a href="//xitu.github.io/tensorflow-docs-web/./mobile/optimizing.html#binary_size">移动端优化</a></p>
<h3>Locate the implementation</h3>
<p>Operations are broken into two parts. The first is the op definition, which declares the signature of the operation, which inputs, outputs, and attributes it has. These take up very little space, and so all are included by default. The implementations of the op computations are done in kernels, which live in the <code>tensorflow/core/kernels</code> folder. You need to compile the C++ file containing the kernel implementation of the op you need into the library. To figure out which file that is, you can search for the operation name in the source files.</p>
<p><a href="https://github.com/search?utf8=%E2%9C%93&amp;q=repo%3Atensorflow%2Ftensorflow+extension%3Acc+path%3Atensorflow%2Fcore%2Fkernels+REGISTER+Mul&amp;type=Code&amp;ref=searchresults">Here’s an example search in github</a>.</p>
<p>You’ll see that this search is looking for the <code>Mul</code> op implementation, and it finds it in <code>tensorflow/core/kernels/cwise_op_mul_1.cc</code>. You need to look for macros beginning with <code>REGISTER</code>, with the op name you care about as one of the string arguments.</p>
<p>In this case, the implementations are actually broken up across multiple <code>.cc</code> files, so you’d need to include all of them in your build. If you’re more comfortable using the command line for code search, here’s a grep command that also locates the right files if you run it from the root of your TensorFlow repository:</p>
<p><code>grep 'REGISTER.*"Mul"' tensorflow/core/kernels/*.cc</code></p>
<h3>Add the implementation to the build</h3>
<p>If you’re using Bazel, and building for Android, you’ll want to add the files you’ve found to the <a href="https://www.tensorflow.org/code/tensorflow/core/kernels/BUILD#L3565"><code>android_extended_ops_group1</code></a> or <a href="https://www.tensorflow.org/code/tensorflow/core/kernels/BUILD#L3632"><code>android_extended_ops_group2</code></a> targets. You may also need to include any .cc files they depend on in there. If the build complains about missing header files, add the .h’s that are needed into the <a href="https://www.tensorflow.org/code/tensorflow/core/kernels/BUILD#L3525"><code>android_extended_ops</code></a> target.</p>
<p>If you’re using a makefile targeting iOS, Raspberry Pi, etc, go to <a href="https://www.tensorflow.org/code/tensorflow/contrib/makefile/tf_op_files.txt"><code>tensorflow/contrib/makefile/tf_op_files.txt</code></a> and add the right implementation files there.</p>

        </main>
    </div>
</div>
<!-- Content end-->

<!-- Footer start -->
<footer class="footer">
    <div class="container">
        <div>如果您发现本页面存在错误或可以改进，请<a href="https://github.com/xitu/tensorflow-docs/blob/zh-hans/mobile/prepare_models.md" target="_blank">点击此处</a>帮助我们改进。本页贡献者：<span id="contributors"></span></div>
        <hr/>
        <div class="text-center official-links">
            <a href="https://www.tensorflow.org"><img
                    src="https://www.tensorflow.org/_static/b1fb9a8564/images/tensorflow/lockup.png" height="20"/></a>
            <a href="https://github.com/xitu/tensorflow-docs"><img
                    src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Logo.png" height="20"></a>
            <a href="https://juejin.im"><img src="//xitu.github.io/tensorflow-docs-web/assets/imgs/logo_app_white.png" height="20"/></a>
        </div>
    </div>
</footer>
<script>
    var contributors = [{'leviding': 'https://avatars3.githubusercontent.com/u/26959437?v=4'}]
</script>
<!-- Footer end -->
</body>
<script src="//cdn.bootcss.com/jquery/3.3.1/jquery.slim.min.js" type="text/javascript"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js" type="text/javascript"></script>
<script src="//xitu.github.io/tensorflow-docs-web/assets/js/main.js" type="text/javascript"></script>
</html>