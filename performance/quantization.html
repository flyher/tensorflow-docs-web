<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html;charset=UTF-8">
    <title>Fixed Point Quantization</title>
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="//xitu.github.io/tensorflow-docs-web/assets/css/main.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
</head>
<body>
<!-- Header start -->
<nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="#">TensorFlow</a>
    <button class="navbar-toggler" type="button" aria-expanded="false" aria-label="Menu"
            onclick="$('.collapse').toggle()">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse">
        <ul class="navbar-nav mr-auto">
        </ul>
        <!-- TODO: Search function-->
        <!--<form class="form-inline my-2 my-lg-0">-->
            <!--<input class="form-control mr-sm-2" type="search" placeholder="Search" aria-label="Search">-->
            <!--<button class="btn btn-outline-success my-2 my-sm-0" type="submit">Search</button>-->
        <!--</form>-->
    </div>
</nav>
<script>
    var head = [{'link': '//xitu.github.io/tensorflow-docs-web/extend/index.html', 'name': '扩展', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/install/index.html', 'name': '安装 TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/deploy/index.html', 'name': '部署', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/about/index.html', 'name': '关于 TensorFlow', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/get_started/index.html', 'name': '开始', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/mobile/index.html', 'name': '概述', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/tutorials/index.html', 'name': '教程', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/javascript/index.html', 'name': 'JavaScript', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/index.html', 'name': '性能', 'selected': 1}, {'link': '//xitu.github.io/tensorflow-docs-web/community/index.html', 'name': '社区', 'selected': 0}, {'link': '//xitu.github.io/tensorflow-docs-web/programmers_guide/index.html', 'name': '开发者指南', 'selected': 0}]
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-92630037-8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-92630037-8');
</script>

<!-- Header end -->

<!-- Content start-->
<div class="container-fluid">
    <div class="row">
        <nav class="col-md-2 d-none d-md-block bg-light sidebar">
    <div class="sidebar-sticky" id="left-nav">

    </div>
</nav>
<script>
    var nav = [{'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/performance/index.html', 'title': '性能'}, {'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/performance/performance_guide.html', 'title': '性能指南'}, {'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/performance/datasets_performance.html', 'title': '输入管道性能指南'}, {'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/performance/performance_models.html', 'title': '高性能模型'}, {'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/performance/benchmarks.html', 'title': '基准'}, {'type': 'child', 'link': '//xitu.github.io/tensorflow-docs-web/performance/quantization.html', 'title': 'Fixed Point Quantization'}, {'type': 'parent', 'title': ' XLA', 'sub_class': [{'link': '//xitu.github.io/tensorflow-docs-web/performance/xla/index.html', 'title': 'XLA 概述'}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/xla/broadcasting.html', 'title': '广播语义'}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/xla/developing_new_backend.html', 'title': '为 XLA 开发一个新后端'}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/xla/jit.html', 'title': '使用即时编译'}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/xla/operation_semantics.html', 'title': '操作语义'}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/xla/shapes.html', 'title': '形状和布局'}, {'link': '//xitu.github.io/tensorflow-docs-web/performance/xla/tfcompile.html', 'title': '使用提前编译'}]}]
</script>
        <main role="main" class="col-12 col-md-9 col-xl-8 py-md-3 pl-md-5 bd-content">
            <h1 id="toc-0">Fixed Point Quantization</h1>
<p>Quantization techniques store and calculate numbers in more compact formats.<br>
<a href="/mobile/tflite/">TensorFlow Lite</a> adds quantization that uses an 8-bit fixed<br>
point representation.</p>
<p>Since a challenge for modern neural networks is optimizing for high accuracy, the<br>
priority has been improving accuracy and speed during training. Using floating<br>
point arithmetic is an easy way to preserve accuracy and GPUs are designed to<br>
accelerate these calculations.</p>
<p>However, as more machine learning models are deployed to mobile devices,<br>
inference efficiency has become a critical issue. Where the computational demand<br>
for <em>training</em> grows with the amount of models trained on different<br>
architectures, the computational demand for <em>inference</em> grows in proportion to<br>
the amount of users.</p>
<h2 id="toc-1">Quantization benefits</h2>
<p>Using 8-bit calculations help your models run faster and use less power. This is<br>
especially important for mobile devices and embedded applications that can't run<br>
floating point code efficiently, for example, Internet of Things (IoT) and<br>
robotics devices. There are additional opportunities to extend this support to<br>
more backends and research lower precision networks.</p>
<h3 id="toc-2">Smaller file sizes {: .hide-from-toc}</h3>
<p>Neural network models require a lot of space on disk. For example, the original<br>
AlexNet requires over 200 MB for the float format—almost all of that for the<br>
model's millions of weights. Because the weights are slightly different<br>
floating point numbers, simple compression formats perform poorly (like zip).</p>
<p>Weights fall in large layers of numerical values. For each layer, weights tend to<br>
be normally distributed within a range. Quantization can shrink file sizes by<br>
storing the minimum and maximum weight for each layer, then compress each<br>
weight's float value to an 8-bit integer representing the closest real number in<br>
a linear set of 256 within the range.</p>
<h3 id="toc-3">Faster inference {: .hide-from-toc}</h3>
<p>Since calculations are run entirely on 8-bit inputs and outputs, quantization<br>
reduces the computational resources needed for inference calculations. This is<br>
more involved, requiring changes to all floating point calculations, but results<br>
in a large speed-up for inference time.</p>
<h3 id="toc-4">Memory efficiency {: .hide-from-toc}</h3>
<p>Since fetching 8-bit values only requires 25% of the memory bandwidth of floats,<br>
more efficient caches avoid bottlenecks for RAM access. In many cases, the power<br>
consumption for running a neural network is dominated by memory access. The<br>
savings from using fixed-point 8-bit weights and activations are significant.</p>
<p>Typically, SIMD operations are available that run more operations per clock<br>
cycle. In some cases, a DSP chip is available that accelerates 8-bit calculations<br>
resulting in a massive speedup.</p>
<h2 id="toc-5">Fixed point quantization techniques</h2>
<p>The goal is to use the same precision for weights and activations during both<br>
training and inference. But an important difference is that training consists of<br>
a forward pass and a backward pass, while inference only uses a forward pass.<br>
When we train the model with quantization in the loop, we ensure that the forward<br>
pass matches precision for both training and inference.</p>
<p>To minimize the loss in accuracy for fully fixed point models (weights and<br>
activations), train the model with quantization in the loop. This simulates<br>
quantization in the forward pass of a model so weights tend towards values that<br>
perform better during quantized inference. The backward pass uses quantized<br>
weights and activations and models quantization as a straight through estimator.<br>
(See Bengio et al., <a href="https://arxiv.org/abs/1308.3432">2013</a>)</p>
<p>Additionally, the minimum and maximum values for activations are determined<br>
during training. This allows a model trained with quantization in the loop to be<br>
converted to a fixed point inference model with little effort, eliminating the<br>
need for a separate calibration step.</p>
<h2 id="toc-6">Quantization training with TensorFlow</h2>
<p><a href="//xitu.github.io/tensorflow-docs-web/api_guides/python/array_ops.html#Fake_quantization">Tensor Transformations</a></p>
<p>Since it's difficult to add these fake quantization operations to all the<br>
required locations in the model, there's a function available that rewrites the<br>
training graph. To create a fake quantized training graph:</p>
<pre><code># Build forward pass of model.
loss = tf.losses.get_total_loss()

# Call the training rewrite which rewrites the graph in-place with
# FakeQuantization nodes and folds batchnorm for training. It is
# often needed to fine tune a floating point model for quantization
# with this training tool. When training from scratch, quant_delay
# can be used to activate quantization after training to converge
# with the float graph, effectively fine-tuning the model.
tf.contrib.quantize.create_training_graph(quant_delay=2000000)

# Call backward pass optimizer as usual.
optimizer = tf.train.GradientDescentOptimizer(learning_rate)
optimizer.minimize(loss)
</code></pre>
<p>The rewritten <em>eval graph</em> is non-trivially different from the <em>training graph</em><br>
since the quantization ops affect the batch normalization step. Because of this,<br>
we've added a separate rewrite for the <em>eval graph</em>:</p>
<pre><code># Build eval model
logits = tf.nn.softmax_cross_entropy_with_logits(...)

# Call the eval rewrite which rewrites the graph in-place with
# FakeQuantization nodes and fold batchnorm for eval.
tf.contrib.quantize.create_eval_graph()

# Save the checkpoint and eval graph proto to disk for freezing
# and providing to TFLite.
with open(eval_graph_file, ‘w’) as f:
  f.write(str(g.as_graph_def()))
saver = tf.train.Saver()
saver.save(sess, checkpoint_name)
</code></pre>
<p>Methods to rewrite the training and eval graphs are an active area of research<br>
and experimentation. Although rewrites and quantized training might not work or<br>
improve performance for all models, we are working to generalize these<br>
techniques.</p>
<h2 id="toc-7">Generating fully quantized models</h2>
<p>The previously demonstrated after-rewrite eval graph only <em>simulates</em><br>
quantization. To generate real fixed point computations from a trained<br>
quantization model, convert it to a fixed point kernel. Tensorflow Lite supports<br>
this conversion from the graph resulting from <code>create_eval_graph</code>.</p>
<p>First, create a frozen graph that will be the input for the TensorFlow Lite<br>
toolchain:</p>
<pre><code>bazel build tensorflow/python/tools:freeze_graph &amp;&amp; \
  bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=eval_graph_def.pb \
  --input_checkpoint=checkpoint \
  --output_graph=frozen_eval_graph.pb --output_node_names=outputs
</code></pre>
<p>Provide this to the TensorFlow Lite Optimizing Converter (TOCO) to get a fully<br>
quantized TensorFLow Lite model:</p>
<pre><code>bazel build tensorflow/contrib/lite/toco:toco &amp;&amp; \
  ./bazel-bin/third_party/tensorflow/contrib/lite/toco/toco \
  --input_file=frozen_eval_graph.pb \
  --output_file=tflite_model.tflite \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --inference_type=QUANTIZED_UINT8 \
  --input_shape="1,224, 224,3" \
  --input_array=input \
  --output_array=outputs \
  --std_value=127.5 --mean_value=127.5
</code></pre>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/quantize"><code>tf.contrib.quantize</code></a></p>
<h2 id="toc-8">Quantized accuracy</h2>
<p>Fixed point <a href="https://arxiv.org/abs/1704.0486">MobileNet</a> models are released with<br>
8-bit weights and activations. Using the rewriters, these models achieve the<br>
Top-1 accuracies listed in Table 1. For comparison, the floating point accuracies<br>
are listed for the same models. The code used to generate these models<br>
<a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md">is available</a><br>
along with links to all of the pretrained mobilenet_v1 models.</p>
<figure>
  <table>
    <tr>
      <th>Image Size</th>
      <th>Depth</th>
      <th>Top-1 Accuracy:<br>Floating point</th>
      <th>Top-1 Accuracy:<br>Fixed point: 8 bit weights and activations</th>
    </tr>
    <tr><td>128</td><td>0.25</td><td>0.415</td><td>0.399</td></tr>
    <tr><td>128</td><td>0.5</td><td>0.563</td><td>0.549</td></tr>
    <tr><td>128</td><td>0.75</td><td>0.621</td><td>0.598</td></tr>
    <tr><td>128</td><td>1</td><td>0.652</td><td>0.64</td></tr>
    <tr><td>160</td><td>0.25</td><td>0.455</td><td>0.435</td></tr>
    <tr><td>160</td><td>0.5</td><td>0.591</td><td>0.577</td></tr>
    <tr><td>160</td><td>0.75</td><td>0.653</td><td>0.639</td></tr>
    <tr><td>160</td><td>1</td><td>0.68</td><td>0.673</td></tr>
    <tr><td>192</td><td>0.25</td><td>0.477</td><td>0.458</td></tr>
    <tr><td>192</td><td>0.5</td><td>0.617</td><td>0.604</td></tr>
    <tr><td>192</td><td>0.75</td><td>0.672</td><td>0.662</td></tr>
    <tr><td>192</td><td>1</td><td>0.7</td><td>0.69</td></tr>
    <tr><td>224</td><td>0.25</td><td>0.498</td><td>0.482</td></tr>
    <tr><td>224</td><td>0.5</td><td>0.633</td><td>0.622</td></tr>
    <tr><td>224</td><td>0.75</td><td>0.684</td><td>0.679</td></tr>
    <tr><td>224</td><td>1</td><td>0.709</td><td>0.697</td></tr>
  </table>
  <figcaption>
    <b>Table 1</b>: MobileNet Top-1 accuracy on Imagenet Validation dataset.
  </figcaption>
</figure><h2 id="toc-9">Representation for quantized tensors</h2>
<p>TensorFlow approaches the conversion of floating-point arrays of numbers into<br>
8-bit representations as a compression problem. Since the weights and activation<br>
tensors in trained neural network models tend to have values that are distributed<br>
across comparatively small ranges (for example, -15 to +15 for weights or -500 to<br>
1000 for image model activations). And since neural nets tend to be robust<br>
handling noise, the error introduced by quantizing to a small set of values<br>
maintains the precision of the overall results within an acceptable threshold. A<br>
chosen representation must perform fast calculations, especially the large matrix<br>
multiplications that comprise the bulk of the computations while running a model.</p>
<p>This is represented with two floats that store the overall minimum and maximum<br>
values corresponding to the lowest and highest quantized value. Each entry in the<br>
quantized array represents a float value in that range, distributed linearly<br>
between the minimum and maximum. For example, with a minimum of -10.0 and maximum<br>
of 30.0f, and an 8-bit array, the quantized values represent the following:</p>
<figure>
  <table>
    <tr><th>Quantized</th><th>Float</th></tr>
    <tr><td>0</td><td>-10.0</td></tr>
    <tr><td>255</td><td>30.0</td></tr>
    <tr><td>128</td><td>10.0</td></tr>
  </table>
  <figcaption>
    <b>Table 2</b>: Example quantized value range
  </figcaption>
</figure><p>The advantages of this representation format are:</p>
<ul>
<li>It efficiently represents an arbitrary magnitude of ranges.</li>
<li>The values don't have to be symmetrical.</li>
<li>The format represents both signed and unsigned values.</li>
<li>The linear spread makes multiplications straightforward.</li>
</ul>
<p>Alternative techniques use lower bit depths by non-linearly distributing the<br>
float values across the representation, but currently are more expensive in terms<br>
of computation time. (See Han et al.,<br>
<a href="https://arxiv.org/abs/1510.00149">2016</a>.)</p>
<p>The advantage of having a clear definition of the quantized format is that it's<br>
always possible to convert back and forth from fixed-point to floating-point for<br>
operations that aren't quantization-ready, or to inspect the tensors for<br>
debugging.</p>

        </main>
        <div class="d-none d-xl-block col-xl-2 bd-toc">
        <ul id="table-of-content">
<li><a href="#toc-0">Fixed Point Quantization</a><ul>
<li><a href="#toc-1">Quantization benefits</a></li>
<li><a href="#toc-5">Fixed point quantization techniques</a></li>
<li><a href="#toc-6">Quantization training with TensorFlow</a></li>
<li><a href="#toc-7">Generating fully quantized models</a></li>
<li><a href="#toc-8">Quantized accuracy</a></li>
<li><a href="#toc-9">Representation for quantized tensors</a></li>
</ul>
</li>
</ul>

        </div>
    </div>
</div>
<!-- Content end-->

<!-- Footer start -->
<footer class="footer">
    <div class="container">
        <div>如果您发现本页面存在错误或可以改进，请<a href="https://github.com/xitu/tensorflow-docs/blob/zh-hans/performance/quantization.md" target="_blank">点击此处</a>帮助我们改进。本页贡献者：<span id="contributors"></span></div>
        <hr/>
        <div class="text-center official-links">
            <a href="https://www.tensorflow.org"><img
                    src="https://www.tensorflow.org/_static/b1fb9a8564/images/tensorflow/lockup.png" height="20"/></a>
            <a href="https://github.com/xitu/tensorflow-docs"><img
                    src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Logo.png" height="20"></a>
            <a href="https://juejin.im"><img src="//xitu.github.io/tensorflow-docs-web/assets/imgs/logo_app_white.png" height="20"/></a>
        </div>
    </div>
</footer>
<script>
    var contributors = [{'leviding': 'https://avatars3.githubusercontent.com/u/26959437?v=4'}]
</script>
<!-- Footer end -->
</body>
<script src="//cdn.bootcss.com/jquery/3.3.1/jquery.slim.min.js" type="text/javascript"></script>
<script src="//cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js" type="text/javascript"></script>
<script src="//xitu.github.io/tensorflow-docs-web/assets/js/main.js" type="text/javascript"></script>
</html>